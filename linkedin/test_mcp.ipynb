{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:ThreadBasedProceduralMemory initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MongoDB persistence\n",
      "\n",
      "--- Testing Graph with All Fixes ---\n",
      "√¢≈ì‚Ä¶ Graph execution successful\n",
      "Messages generated: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import operator\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import traceback\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "import functools\n",
    "import logging\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # <-- MUST be at the very top before any os.getenv\n",
    "\n",
    "APIFY_TOKEN = os.getenv(\"APIFY_API_KEY\")  #\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Circuit breaker state\n",
    "circuit_open = False\n",
    "failure_count = 0\n",
    "failure_threshold = 3  # Open circuit after 3 consecutive failures\n",
    "circuit_reset_time = 30  # seconds\n",
    "last_failure_time = 0\n",
    "\n",
    "\n",
    "def llm_call_with_retry_circuit(prompt: str, max_retries=3, retry_delay=2):\n",
    "    \"\"\"\n",
    "    Wrapper for model.generate_content with:\n",
    "    - Retry on transient errors\n",
    "    - Circuit breaker to stop repeated failures\n",
    "    - Token/cost logging\n",
    "    \"\"\"\n",
    "    global circuit_open, failure_count, last_failure_time\n",
    "\n",
    "    # Circuit breaker check\n",
    "    if circuit_open:\n",
    "        if time.time() - last_failure_time < circuit_reset_time:\n",
    "            logging.warning(\"Circuit open. Skipping LLM call.\")\n",
    "            raise RuntimeError(\"Circuit open due to repeated failures\")\n",
    "        else:\n",
    "            logging.info(\"Resetting circuit breaker.\")\n",
    "            circuit_open = False\n",
    "            failure_count = 0\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            res = model.generate_content(prompt)\n",
    "\n",
    "            # Logging token/cost info if available\n",
    "            if hasattr(res, \"usage\"):\n",
    "                tokens = res.usage.get(\"total_tokens\", \"N/A\")\n",
    "                cost = tokens * 0.00001  # Example: adjust based on model pricing\n",
    "                logging.info(\n",
    "                    f\"LLM call successful | Tokens used: {tokens} | Est. cost: ${cost:.6f}\"\n",
    "                )\n",
    "\n",
    "            # Reset failure count on success\n",
    "            failure_count = 0\n",
    "            return res\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"LLM call failed on attempt {attempt}: {e}\")\n",
    "            failure_count += 1\n",
    "            last_failure_time = time.time()\n",
    "            if failure_count >= failure_threshold:\n",
    "                circuit_open = True\n",
    "                logging.error(\n",
    "                    f\"Circuit opened after {failure_count} consecutive failures.\"\n",
    "                )\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                raise RuntimeError(f\"LLM call failed after {max_retries} attempts: {e}\")\n",
    "\n",
    "\n",
    "# Try to use MongoDB persistence, fallback to memory if not available\n",
    "try:\n",
    "    from langgraph.checkpoint.mongodb import MongoDBSaver\n",
    "\n",
    "    mongo_url = os.getenv(\"MONGODB_URI\", \"mongodb://localhost:27017\")\n",
    "\n",
    "    # --- Explicit connection check ---\n",
    "    client = MongoClient(mongo_url, serverSelectionTimeoutMS=2000)\n",
    "    client.admin.command(\"ping\")  # will raise if not connected\n",
    "\n",
    "    db_name = \"career_bot\"\n",
    "    collection_name = \"checkpoints\"\n",
    "    db = client[\"career_bot\"]\n",
    "    collection = db[\"checkpoints\"]\n",
    "\n",
    "    # --- Force creation of DB + collection by inserting a dummy if empty ---\n",
    "    if collection.count_documents({}) == 0:\n",
    "        collection.insert_one({\"_init\": True})\n",
    "        print(f\"Created DB '{db_name}' and collection '{collection_name}'\")\n",
    "\n",
    "    # --- LangGraph Saver ---\n",
    "    memory = MongoDBSaver(\n",
    "        client=client, db_name=\"career_bot\", collection_name=\"checkpoints\"\n",
    "    )\n",
    "\n",
    "    print(\"Using MongoDB persistence\")\n",
    "\n",
    "except Exception as e:\n",
    "    from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "    memory = MemorySaver()\n",
    "    tb = traceback.format_exc()\n",
    "    print(f\"Falling back to in-memory persistence: {e}|trace={tb}\")\n",
    "\n",
    "\n",
    "import google.generativeai as genai\n",
    "from scraper_utils import scrape_and_clean_profile\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
    "\n",
    "\n",
    "def _log_transition(thread_id: str, node: str, action: str, note: str = \"\"):\n",
    "    try:\n",
    "        logging.info(f\"[flow] thread={thread_id} node={node} action={action} {note}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "class AgentState(TypedDict, total=False):\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    profile_data: Annotated[dict, lambda _, x: x]\n",
    "    current_job_description: Annotated[Optional[str], lambda _, x: x]\n",
    "    linkedin_url: Annotated[Optional[str], lambda _, x: x]\n",
    "    thread_id: Annotated[Optional[str], lambda _, x: x]  \n",
    "    websearch_results: Annotated[list, lambda _, x: x]\n",
    "    websearch_summary: Annotated[list, lambda _, x: x]\n",
    "    # FIX 1: Add flag to track if profile was already scraped\n",
    "    profile_scraped: Annotated[bool, lambda _, x: x]\n",
    "\n",
    "\n",
    "from apify_client import ApifyClient\n",
    "import time\n",
    "\n",
    "# FIX 3: Improved websearch with better error handling and link extraction\n",
    "def websearch_mcp_node(state: AgentState) -> dict:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"websearch_mcp\", \"start\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    messages.append(AIMessage(\"üîç Fetching web data...\"))\n",
    "\n",
    "    # extract latest human query\n",
    "    query = \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            query = msg.content.strip()\n",
    "            break\n",
    "\n",
    "    if not query:\n",
    "        messages.append(AIMessage(\"‚ö† No query provided for web search.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"websearch_mcp\", \"done\", \"no-query\")\n",
    "        return state\n",
    "\n",
    "    api_token = os.getenv(\"APIFY_API_KEY\")\n",
    "    if not api_token:\n",
    "        messages.append(AIMessage(\"‚ùå Apify API token not configured.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"websearch_mcp\", \"done\", \"no-token\")\n",
    "        return state\n",
    "\n",
    "    client = ApifyClient(api_token)\n",
    "    actor_id = \"WMg2EXLzJGPVQ5Vfq\"   # √¢≈ì‚Ä¶ stick with the tested actor\n",
    "\n",
    "    input_data = {\n",
    "        \"query\": query,\n",
    "        \"num\": 1,\n",
    "        \"start\": 1\n",
    "    }\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 2\n",
    "    results = []\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            logging.info(f\"Starting Apify web search for: {query}\")\n",
    "            run = client.actor(actor_id).call(run_input=input_data)\n",
    "\n",
    "            dataset_items = client.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
    "            print(dataset_items)\n",
    "            logging.info(f\"Retrieved {len(dataset_items)} items from Apify\")\n",
    "\n",
    "            for item in dataset_items:\n",
    "                result = {\n",
    "                    \"title\": item.get(\"title\", \"\"),\n",
    "                    \"snippet\": item.get(\"snippet\", \"\"),\n",
    "                    \"link\": item.get(\"link\", \"\"),\n",
    "                    \"date\": item.get(\"date\")\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "            print(\"printing search results:\", results)\n",
    "\n",
    "\n",
    "            if results:\n",
    "                messages.append(\n",
    "                    AIMessage(\n",
    "                        f\"√∞≈∏‚Äù¬ç WebSearch results fetched ({len(results)} results):\\n\"\n",
    "                        + \"\\n\".join([f\"- {r['title']}: {r['link']}\" for r in results[:3]])\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                messages.append(AIMessage(\"‚ö† No search results found.\"))\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Apify search attempt {attempt} failed: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                messages.append(AIMessage(f\"‚ö† Apify attempt {attempt} failed. Retrying...\"))\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                messages.append(AIMessage(f\"‚ùå Apify WebSearch failed after {max_retries} attempts: {e}\"))\n",
    "\n",
    "    state[\"messages\"] = messages\n",
    "    state[\"websearch_results\"] = results\n",
    "    state[\"web_search\"] = results\n",
    "    _log_transition(thread_id, \"websearch_mcp\", \"done\", f\"results={len(results)}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def enrich_websearch_node(state):\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"enrich_websearch\", \"start\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    results = state.get(\"websearch_results\", [])\n",
    "    \n",
    "    if not results:\n",
    "        messages.append(AIMessage(\"‚ö† No search results to enrich.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"enrich_websearch\", \"done\", \"no-results\")\n",
    "        return state\n",
    "\n",
    "    summary = []\n",
    "    for r in results:\n",
    "        # Use snippet if available, otherwise try to fetch content\n",
    "        content = r.get(\"snippet\") or r.get(\"content\") or r.get(\"description\", \"\")\n",
    "        \n",
    "        if not content and r.get(\"link\"):\n",
    "            # Fallback to simple content fetch\n",
    "            try:\n",
    "                import requests\n",
    "                response = requests.get(r[\"link\"], timeout=10, headers={\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "                })\n",
    "                content = response.text[:2000]  # Limit content\n",
    "            except Exception as e:\n",
    "                content = f\"Could not fetch content: {e}\"\n",
    "\n",
    "        # Summarize content\n",
    "        try:\n",
    "            if content and len(content) > 50:\n",
    "                summary_prompt = f\"Summarize the key points for career planning from this content:\\n\\n{content[:1500]}\"\n",
    "                res = llm_call_with_retry_circuit(summary_prompt)\n",
    "                summary_text = res.text\n",
    "            else:\n",
    "                summary_text = content or \"No content available\"\n",
    "                \n",
    "            summary.append({\n",
    "                \"title\": r[\"title\"], \n",
    "                \"link\": r[\"link\"], \n",
    "                \"summary\": summary_text[:500]  # Limit summary length\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error enriching search result: {e}\")\n",
    "            summary.append({\n",
    "                \"title\": r[\"title\"], \n",
    "                \"link\": r[\"link\"], \n",
    "                \"summary\": f\"√¢¬ù≈í Could not summarize: {e}\"\n",
    "            })\n",
    "\n",
    "    messages.append(AIMessage(f\"‚úÖ Search results enriched with AI summaries ({len(summary)} results).\"))\n",
    "    state[\"messages\"] = messages\n",
    "    state[\"websearch_summary\"] = summary\n",
    "    _log_transition(thread_id, \"enrich_websearch\", \"done\", f\"summary={len(summary)}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def store_websearch_node(state: AgentState) -> dict:\n",
    "    \"\"\"No-op for DB. Keep websearch_summary in state only; storage happens in plan/review nodes.\"\"\"\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"store_websearch\", \"done\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FIX 1: Improved LinkedIn scraper that only runs when needed\n",
    "def linkedin_scraper_node(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Scrapes LinkedIn profile URL only if not already scraped in this session.\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    profile_scraped = state.get(\"profile_scraped\", False)\n",
    "    existing_profile = state.get(\"profile_data\")\n",
    "\n",
    "    # FIX 1: Skip scraping if profile already exists and was scraped\n",
    "    _log_transition(thread_id, \"linkedin_scraper\", \"start\")\n",
    "    if profile_scraped and existing_profile:\n",
    "        logging.info(\"Profile already scraped in this session, skipping scraper\")\n",
    "        _log_transition(thread_id, \"linkedin_scraper\", \"goto\", \"career_qa_router\")\n",
    "        return Command(goto=[\"career_qa_router\"], update=state)\n",
    "\n",
    "    linkedin_url = state.get(\"linkedin_url\", \"\").strip()\n",
    "\n",
    "    if not linkedin_url:\n",
    "        messages.append(AIMessage(\"Please provide a LinkedIn profile URL to begin.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"linkedin_scraper\", \"interrupt\", \"awaiting URL\")\n",
    "        return interrupt(\"await_input\")\n",
    "\n",
    "    messages.append(AIMessage(\"Scraping your LinkedIn profile...\"))\n",
    "\n",
    "    try:\n",
    "        scraped_profile = scrape_and_clean_profile(\n",
    "            linkedin_url=linkedin_url, api_token=os.getenv(\"APIFY_API_KEY\")\n",
    "        )\n",
    "\n",
    "        if not scraped_profile:\n",
    "            messages.append(AIMessage(\"Failed to extract profile. Try again.\"))\n",
    "            state[\"messages\"] = messages\n",
    "            _log_transition(thread_id, \"linkedin_scraper\", \"interrupt\", \"scrape failed\")\n",
    "            return interrupt(\"await_input\")\n",
    "\n",
    "        messages.append(AIMessage(\"Profile successfully scraped!\"))\n",
    "        state[\"profile_data\"] = scraped_profile\n",
    "        state[\"profile_scraped\"] = True  # FIX 1: Mark as scraped\n",
    "        state[\"messages\"] = messages\n",
    "        state[\"thread_id\"] = thread_id\n",
    "\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"Error scraping LinkedIn profile: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"linkedin_scraper\", \"interrupt\", \"exception during scrape\")\n",
    "        return interrupt(\"await_input\")\n",
    "\n",
    "    _log_transition(thread_id, \"linkedin_scraper\", \"goto\", \"career_qa_router\")\n",
    "    return Command(goto=[\"career_qa_router\"], update=state)\n",
    "\n",
    "\n",
    "def career_qa_router(state: AgentState) -> Command:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    profile = state.get(\"profile_data\", {})\n",
    "    jd = state.get(\"current_job_description\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    _log_transition(thread_id, \"career_qa_router\", \"start\")\n",
    "    question = \"\"\n",
    "\n",
    "    # Determine last speaker and latest human question\n",
    "    last_is_human = False\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, AIMessage):\n",
    "            last_is_human = False\n",
    "            break\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            question = msg.content.strip()\n",
    "            last_is_human = True\n",
    "            break\n",
    "\n",
    "    # If profile not scraped yet, route to scraper once\n",
    "    if not state.get(\"profile_scraped\") or not state.get(\"profile_data\"):\n",
    "        if state.get(\"linkedin_url\", \"\").strip():\n",
    "            _log_transition(thread_id, \"career_qa_router\", \"goto\", \"linkedin_scraper\")\n",
    "            return Command(goto=\"linkedin_scraper\", update=state)\n",
    "        # Ask for URL\n",
    "        messages.append(AIMessage(\"Please provide a LinkedIn profile URL to begin.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"career_qa_router\", \"interrupt\", \"awaiting linkedin url\")\n",
    "        return interrupt(\"await_input\")\n",
    "\n",
    "    # If last message is not from human, wait for next input\n",
    "    if not last_is_human or not question:\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"career_qa_router\", \"interrupt\", \"awaiting next question\")\n",
    "        return interrupt(\"await_input\")\n",
    "\n",
    "    if question.lower() in {\"quit\", \"exit\", \"stop\"}:\n",
    "        messages.append(AIMessage(\"Okay, ending the conversation.\"))\n",
    "        _log_transition(thread_id, \"career_qa_router\", \"goto\", \"END\")\n",
    "        return Command(goto=END, update={\"messages\": messages})\n",
    "\n",
    "    if \"job description:\" in question.lower():\n",
    "        messages.append(AIMessage(\"Got your new job description.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        state[\"current_job_description\"] = question\n",
    "        _log_transition(thread_id, \"career_qa_router\", \"interrupt\", \"job description captured\")\n",
    "        return interrupt(\"await_input\")\n",
    "\n",
    "    \n",
    "\n",
    "    history = \"\\n\".join(\n",
    "        f\"Human: {m.content}\" if isinstance(m, HumanMessage) else f\"AI: {m.content}\"\n",
    "        for m in messages[-5:]\n",
    "    )\n",
    "    prompt = f\"\"\"\n",
    "    {history}\n",
    "You are an expert routing agent. Decide which module should handle the user's latest question. Understand what the user is asking exactly and use that understanding to make an informed routing decision.\n",
    "\n",
    "Return one of:\n",
    "- analyze_profile\n",
    "- job_fit_agent\n",
    "- enhance_profile\n",
    "- general_qa\n",
    "\n",
    "DO NOT GUESS. Use the following rules:\n",
    "\n",
    "---\n",
    "\n",
    "ROUTE TO: analyze_profile\n",
    "If the user wants a LinkedIn/resume/profile review, feedback, strengths, weaknesses, or audit. Use this if the user is seeking insights or improvements on their LinkedIn profile.\n",
    "\n",
    "Examples:\n",
    "- \"Can you review my LinkedIn?\"\n",
    "- \"What are my strengths and weaknesses?\"\n",
    "- \"Audit my profile\"\n",
    "- Or any other question that implies analyzing the profile.\n",
    "---\n",
    "\n",
    "ROUTE TO: job_fit_agent:\n",
    "If the user wants to assess their fit for a specific job or role. Only route here if a job description was recently provided. And if the user is referencing it in their question.\n",
    "\n",
    "If the user says anything like:\n",
    "- \"Does my profile match this JD?\"\n",
    "- \"Am I eligible for this job?\"\n",
    "- \"Score me against this role\"\n",
    "- Or any other question that implies matching the profile to a job description.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "ROUTE TO: enhance_profile\n",
    "If the user is asking you to audit or improve their profile or a part of their profile. Understand their specific needs and context properly.\n",
    "\n",
    "If the user asks for:\n",
    "- Rewriting/resume improvement\n",
    "- Profile optimization\n",
    "- \"Improve my About section\"\n",
    "- \"Rewrite my Experience bullets\"\n",
    "- Or any other question that implies enhancing the profile.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "ROUTE TO: general_qa\n",
    "If the user has any other general career questions. If none of the above 3 routes apply, use this. Answer it precisely and informatively.\n",
    "\n",
    "Examples:\n",
    "- \"What kind of roles should I target?\"\n",
    "- \"How do I switch fields?\"\n",
    "- \"What are good certifications for data science?\"\n",
    "- \"How do I get into startups?\"\n",
    "- Or any other question that doesn't fit the above categories.\n",
    "\n",
    "---\n",
    "\n",
    "USER QUESTION:\n",
    "{question}\n",
    "\n",
    "Just respond with ONE of:\n",
    "analyze_profile, job_fit_agent, enhance_profile, general_qa\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        result = llm_call_with_retry_circuit(prompt)\n",
    "        decision = result.text.strip().lower()\n",
    "        # Normalize common phrasing to decisions\n",
    "        valid_nodes = {\n",
    "            \"analyze_profile\",\n",
    "            \"job_fit_agent\",\n",
    "            \"enhance_profile\",\n",
    "            \"general_qa\",\n",
    "        }\n",
    "        if decision in valid_nodes:\n",
    "            # Good, we trust it\n",
    "            return Command(goto=decision if decision != \"general_qa\" else \"general_qa_node\",\n",
    "                        update=state)\n",
    "        else:\n",
    "            # Fail gracefully\n",
    "            messages.append(AIMessage(\"‚ö†Ô∏è Sorry, I couldn't decide where to route this.\"))\n",
    "            state[\"messages\"] = messages\n",
    "            _log_transition(thread_id, \"career_qa_router\", \"interrupt\", f\"invalid decision: {decision}\")\n",
    "            return interrupt()\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"‚ö†Ô∏è  Routing error: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"career_qa_router\", \"interrupt\", \"routing exception\")\n",
    "        return interrupt()\n",
    "\n",
    "\n",
    "def analyze_profile_node(state: AgentState) -> dict:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"analyze_profile\", \"start\")\n",
    "    profile = state.get(\"profile_data\")\n",
    "    messages = state[\"messages\"]\n",
    "    if not profile:\n",
    "        messages.append(AIMessage(\"√¢≈°  No profile data found to analyze.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        return interrupt()\n",
    "\n",
    "    profile_text = \"\\n\".join(f\"{k}: {v}\" for k, v in profile.items())\n",
    "    history = \"\\n\".join(\n",
    "        f\"Human: {m.content}\" if isinstance(m, HumanMessage) else f\"AI: {m.content}\"\n",
    "        for m in messages[-5:]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{history}\n",
    "        You are a highly experienced career coach and tech recruiter who has reviewed over 10,000 LinkedIn profiles.\n",
    "\n",
    "        Your job is to critically evaluate the following LinkedIn profile and return a brutally honest, section-wise analysis.\n",
    "\n",
    "        Use the following criteria for evaluation:\n",
    "        1. Clarity and professionalism of writing\n",
    "        2. Technical and strategic relevance of content\n",
    "        3. Recruiter impression: Would you shortlist this profile?\n",
    "\n",
    "        ---\n",
    "\n",
    "        {profile_text}\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### Return output in the following structure:\n",
    "\n",
    "        # LinkedIn Profile Audit\n",
    "\n",
    "        ## Strengths  \n",
    "        List 3√¢‚Ç¨‚Äú5 strengths that stand out across the profile.\n",
    "\n",
    "        ## Weaknesses  \n",
    "        List 3√¢‚Ç¨‚Äú5 major weaknesses holding the profile back.\n",
    " ## Section-by-Section Evaluation  \n",
    "        For each section (About, Experience, Projects, Education, Skills, etc.), write:\n",
    "\n",
    "        - Give a quality score: √¢≈ì‚Ä¶ Strong / √¢≈°  Needs improvement / √¢¬ù≈í Missing  \n",
    "        - Provide 2√¢‚Ç¨‚Äú3 suggestions to improve the section (content, phrasing, structure)  \n",
    "        - Use clean formatting: bold headings, bullet points, and avoid unnecessary repetition.\n",
    "\n",
    "        Constraints:\n",
    "        - Max 4 bullet points per section  \n",
    "        - Each bullet: <30 words  \n",
    "        - Total section feedback: <100 words\n",
    "\n",
    "        ## Top 3 Improvements You Must Make Now  \n",
    "        Each point must be:\n",
    "        - Brutally specific  \n",
    "        - Directly actionable  \n",
    "        - One line only\n",
    "        \n",
    "        Precautions:\n",
    "        - Do not hallucinate and stay within context.\n",
    "        - If the user asks for/about a specific section (e.g., \"improve my projects\"), focus only on that section.\n",
    "        \n",
    "        Begin now.\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        \n",
    "    res = None\n",
    "\n",
    "\n",
    "    try:\n",
    "        res = llm_call_with_retry_circuit(prompt)\n",
    "        messages.append(AIMessage(res.text))\n",
    "        state[\"messages\"] = messages\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"√¢¬ù≈í Error: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "        \n",
    "    if res:\n",
    "        try:\n",
    "            user_query = extract_latest_user_query(messages)\n",
    "            thread_procedural_memory.store_thread_data(\n",
    "                thread_id=thread_id,\n",
    "                profile_data=profile,\n",
    "                user_query=user_query,\n",
    "                ai_response=res.text,\n",
    "                user_id=get_user_id_from_profile(profile)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store QA graph output: {e}\")\n",
    "\n",
    "    _log_transition(thread_id, \"analyze_profile\", \"goto\", \"career_qa_router\")\n",
    "    return Command(goto=\"career_qa_router\", update=state)\n",
    "\n",
    "\n",
    "def job_fit_agent_node(state: AgentState) -> dict:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"job_fit_agent\", \"start\")\n",
    "    profile = state.get(\"profile_data\")\n",
    "    jd = state.get(\"current_job_description\", \"\")\n",
    "    messages = state[\"messages\"]\n",
    "    if not profile or not jd:\n",
    "        messages.append(AIMessage(\"√¢≈°  Missing profile or job description.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        return interrupt(\"await_input\")\n",
    "\n",
    "    profile_text = \"\\n\".join(f\"{k}: {v}\" for k, v in profile.items())\n",
    "    history = \"\\n\".join(\n",
    "        f\"Human: {m.content}\" if isinstance(m, HumanMessage) else f\"AI: {m.content}\"\n",
    "        for m in messages[-5:]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{history}\n",
    "        You are a highly experienced AI Job Fit Evaluator trained on thousands of hiring decisions across several job roles.\n",
    "        Your job is to evaluate how well the candidate's profile matches the given job description.       \n",
    "        ---\n",
    "\n",
    "        JOB DESCRIPTION:\n",
    "        {jd}\n",
    "\n",
    "        CANDIDATE PROFILE:\n",
    "        {profile_text}\n",
    "\n",
    "        ---\n",
    "\n",
    "        TASKS:\n",
    "        1. Evaluate the fitness of the candidate for this job role using industry-standard evaluation practices (skills, experience, keywords, impact, achievements, and alignment).\n",
    "        2. Return a Job Match Score out of 100, and explain how you arrived at it with specific reasoning.\n",
    "        3. List 3√¢‚Ç¨‚Äú5 strengths from the candidate's profile that match the job expectations.\n",
    "        4. Suggest 3√¢‚Ç¨‚Äú5 concrete improvements √¢‚Ç¨‚Äú these could include skill gaps, experience tweaks, weak areas in phrasing, or missing proof of impact.\n",
    "  5. Only evaluate against the given job role. Do not assume adjacent job titles are valid matches.\n",
    "        6. If the candidate seems overqualified or underqualified, clearly state it and explain how that affects the match.\n",
    "\n",
    "        ---\n",
    "\n",
    "        OUTPUT FORMAT:\n",
    "        # √∞≈∏≈Ω¬Ø Job Fit Evaluation\n",
    "\n",
    "        ## √¢≈ì‚Ä¶ Job Match Score: XX/100\n",
    "        - One-line explanation of the score.\n",
    "        - 2√¢‚Ç¨‚Äú3 bullets with specific justification.\n",
    "\n",
    "        ## √∞≈∏≈∏¬© Strengths\n",
    "        - Point 1 (aligned with JD)\n",
    "        - Point 2\n",
    "        - Point 3  \n",
    "        (Each point √¢‚Ä∞¬§ 40 words)\n",
    "\n",
    " ## √∞≈∏≈∏¬• Weaknesses\n",
    "        - specify the top 3-4 points as to why this profile doesn't match the job or will get rejected even if applied and this analysis must be honest and brutal\n",
    "        (Each point √¢‚Ä∞¬§ 40 words)\n",
    "\n",
    "        ## √∞≈∏‚Ä∫  Improvements to Increase Match Score\n",
    "        - Point 1 (what to improve and how)\n",
    "        - Point 2\n",
    "        - Point 3  \n",
    "        (Each point √¢‚Ä∞¬§ 25 words)\n",
    "\n",
    "        ## √∞≈∏‚Äú≈í Verdict\n",
    "        Clearly say if the candidate is a strong match, weak match, or needs improvement to apply. Give a one-liner summary.\n",
    "        \n",
    "        Precautions:\n",
    "        - Do not hallucinate and stay within context.\n",
    "        - If the user asks for/about a specific section (e.g., \"improve my projects\"), focus only on that section.\n",
    "        \n",
    "        Begin now.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "    res = None\n",
    "\n",
    "\n",
    "    try:\n",
    "        res = llm_call_with_retry_circuit(prompt)\n",
    "\n",
    "        messages.append(AIMessage(res.text))\n",
    "        state[\"messages\"] = messages\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"√¢¬ù≈í Error: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "    \n",
    "    if res:\n",
    "        try:\n",
    "            user_query = extract_latest_user_query(messages)\n",
    "            thread_procedural_memory.store_thread_data(\n",
    "                thread_id=thread_id,\n",
    "                profile_data=profile,\n",
    "                user_query=user_query,\n",
    "                ai_response=res.text,\n",
    "                user_id=get_user_id_from_profile(profile)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store QA graph output: {e}\")\n",
    "\n",
    "    _log_transition(thread_id, \"job_fit_agent\", \"goto\", \"career_qa_router\")\n",
    "    return Command(goto=\"career_qa_router\", update=state)\n",
    "\n",
    "\n",
    "def enhance_profile_node(state: AgentState) -> dict:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"enhance_profile\", \"start\")\n",
    "    profile = state.get(\"profile_data\")\n",
    "    jd = state.get(\"current_job_description\", \"\")\n",
    "    messages = state[\"messages\"]\n",
    "    if not profile:\n",
    "        messages.append(AIMessage(\"√¢≈°  No profile found to enhance.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        return interrupt()\n",
    "\n",
    "    profile_text = \"\\n\".join(f\"{k}: {v}\" for k, v in profile.items())\n",
    "    history = \"\\n\".join(\n",
    "        f\"Human: {m.content}\" if isinstance(m, HumanMessage) else f\"AI: {m.content}\"\n",
    "        for m in messages[-5:]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{history}\n",
    "        You are a highly experienced LinkedIn Profile Optimization AI, trained on millions of real-world hiring patterns across top companies like Google, Amazon, Meta, and startups. Your task is to analyze and rewrite the user's LinkedIn profile to improve clarity, strength, and impact.\n",
    "\n",
    "        CONTEXT:\n",
    "        - The user may or may not have shared a job description.\n",
    "        - They might be asking to improve a specific section only (e.g., \"improve my projects to match this JD\").\n",
    "        - You should infer goals from the user's question and adjust accordingly.\n",
    "        - If any question is related to the job description/JD then consider that and then provide an output\n",
    "\n",
    "        CURRENT PROFILE:\n",
    "        {profile_text}  \n",
    "\n",
    "        JOB DESCRIPTION/JD:\n",
    "        {jd}\n",
    "  TASKS:\n",
    "        1. Identify weak sections and rewrite them to be stronger, more professional, and better aligned with either:\n",
    "            - the job description (if provided), or\n",
    "            - general hiring best practices (if no JD is given).\n",
    "        2. Preserve all factual details. Do NOT add imaginary experiences.\n",
    "        3. Use bullet points only where appropriate (e.g., Experience, Projects).\n",
    "        4. Each bullet must be √¢‚Ä∞¬§ 25 words, and **max 4 bullets per section.\n",
    "        5. For the \"About\" section, limit to 2√¢‚Ç¨‚Äú3 tight paragraphs, total **√¢‚Ä∞¬§ 250 words.\n",
    "        6. Add impactful verbs, metrics, and proof of value wherever possible.\n",
    "        7. If the user requested only a section enhancement (e.g., just projects), modify only that section.\n",
    "        8. If the user asks for a specific section (e.g., \"improve my projects\"), focus only on that section, DO NOT TALK ABOUT OTHER SECTIONS.\n",
    "\n",
    "        FORMAT:\n",
    "        Return the improved sections in clean Markdown format.\n",
    "        - Use bold section titles.\n",
    "        - Show only modified sections √¢‚Ç¨‚Äú skip untouched ones to save tokens.\n",
    "        - Make sure the rewritten content feels real, focused, and hiring-ready.\n",
    "\n",
    "Precautions:\n",
    "        - Do not hallucinate and stay within context.\n",
    "        - If the user asks for/about a specific section (e.g., \"improve my projects\"), focus only on that section.\n",
    "\n",
    "        Begin now.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "    res = None\n",
    "\n",
    "\n",
    "    try:\n",
    "        res = llm_call_with_retry_circuit(prompt)\n",
    "\n",
    "        messages.append(AIMessage(res.text))\n",
    "        state[\"messages\"] = messages\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"√¢¬ù≈í Error: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "        \n",
    "        \n",
    "    if res:\n",
    "        try:\n",
    "            user_query = extract_latest_user_query(messages)\n",
    "            thread_procedural_memory.store_thread_data(\n",
    "                thread_id=thread_id,\n",
    "                profile_data=profile,\n",
    "                user_query=user_query,\n",
    "                ai_response=res.text,\n",
    "                user_id=get_user_id_from_profile(profile)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store QA graph output: {e}\")\n",
    "\n",
    "    _log_transition(thread_id, \"enhance_profile\", \"goto\", \"career_qa_router\")\n",
    "    return Command(goto=\"career_qa_router\", update=state)\n",
    "\n",
    "\n",
    "def general_qa_node(state: AgentState) -> dict:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"general_qa_node\", \"start\")\n",
    "    profile = state.get(\"profile_data\")\n",
    "    jd = state.get(\"current_job_description\", \"\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            question = msg.content.strip()\n",
    "            break\n",
    "\n",
    "    history = \"\\n\".join(\n",
    "        f\"Human: {m.content}\" if isinstance(m, HumanMessage) else f\"AI: {m.content}\"\n",
    "        for m in messages[-5:]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful, concise, and highly experienced career guidance assistant.\n",
    "\n",
    "Your task is to answer general career-related questions from users\n",
    "\n",
    "The user may ask about:\n",
    "- Career advice\n",
    "- Interview preparation\n",
    "- Certifications\n",
    "- Job search strategy\n",
    "- Skill-building\n",
    "- Remote work\n",
    "- Career switches\n",
    "- Industry trends\n",
    "- Anything else loosely related to career growth\n",
    "\n",
    "---\n",
    "\n",
    "USER CONTEXT (optional):\n",
    "{history}\n",
    "\n",
    "PROFILE DATA:\n",
    "{state.get(\"profile_data\", \"N/A\")}\n",
    "\n",
    "JOB DESCRIPTION (if any):\n",
    "{state.get(\"current_job_description\", \"N/A\")}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "---\n",
    "\n",
    "### Answer Guidelines:\n",
    "\n",
    "- Answer clearly and concisely.\n",
    "- Prioritize useful, actionable advice.\n",
    "- If the question is vague or broad, ask a clarifying follow-up.\n",
    "- Keep the tone supportive but professional.\n",
    "- Do not suggest uploading a resume or LinkedIn again.\n",
    "- If you detect the user is stressed, confused, or unsure, acknowledge that supportively.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Format:\n",
    "\n",
    "Respond in clean text. Use bullet points or short paragraphs where needed.\n",
    "\n",
    "Start now.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "    res = None\n",
    "\n",
    "\n",
    "    try:\n",
    "        res = llm_call_with_retry_circuit(prompt)\n",
    "\n",
    "        messages.append(AIMessage(res.text))\n",
    "        state[\"messages\"] = messages\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"√¢¬ù≈í Error: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "\n",
    "    \n",
    "    if res:\n",
    "        try:\n",
    "            user_query = extract_latest_user_query(messages)\n",
    "            thread_procedural_memory.store_thread_data(\n",
    "                thread_id=thread_id,\n",
    "                profile_data=profile,\n",
    "                user_query=user_query,\n",
    "                ai_response=res.text,\n",
    "                user_id=get_user_id_from_profile(profile)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store QA graph output: {e}\")\n",
    "\n",
    "    _log_transition(thread_id, \"general_qa_node\", \"goto\", \"career_qa_router\")\n",
    "    return Command(goto=\"career_qa_router\", update=state)\n",
    "\n",
    "\n",
    "# Import remaining modules as in original...\n",
    "# (ThreadBasedProceduralMemory class and other imports remain the same)\n",
    "\n",
    "# ThreadBasedProceduralMemory class - keep as is from original\n",
    "class ThreadBasedProceduralMemory:\n",
    "    \"\"\"Enhanced procedural learning with thread-based storage in MongoDB\"\"\"\n",
    "\n",
    "    def __init__(self, mongo_url: str = None, db_name: str = \"career_bot\"):\n",
    "        self.mongo_url = mongo_url or os.getenv(\"MONGODB_URI\", \"mongodb://localhost:27017\")\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = \"procedural_threads\"\n",
    "        self._client = MongoClient(self.mongo_url) \n",
    "        self._collection = None\n",
    "        self._initialize_connection()\n",
    "    \n",
    "    def _initialize_connection(self):\n",
    "        \"\"\"Initialize MongoDB connection\"\"\"\n",
    "        try:\n",
    "            self._client = MongoClient(self.mongo_url, serverSelectionTimeoutMS=5000)\n",
    "            self._client.admin.command(\"ping\")\n",
    "            db = self._client[self.db_name]\n",
    "            self._collection = db[self.collection_name]\n",
    "            \n",
    "            # Create indexes for efficient querying\n",
    "            self._collection.create_index(\"thread_id\")\n",
    "            self._collection.create_index(\"user_id\") \n",
    "            self._collection.create_index([(\"conversations.user_query\", \"text\")])\n",
    "            \n",
    "            logging.info(\"ThreadBasedProceduralMemory initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize MongoDB: {e}\")\n",
    "            self._client = None\n",
    "            self._collection = None\n",
    "    \n",
    "    def _is_connected(self) -> bool:\n",
    "        \"\"\"Check MongoDB connection - FIXED\"\"\"\n",
    "        if self._client is None or self._collection is None:\n",
    "            return False\n",
    "        try:\n",
    "            self._client.admin.command(\"ping\")\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def store_thread_data(self, thread_id: str, profile_data: dict, \n",
    "                         user_query: str, ai_response: str, user_id: str = None,\n",
    "                         web_search: Optional[list] = None) -> bool:\n",
    "        \"\"\"Store or update thread data with profile and conversation - FIXED\"\"\"\n",
    "        if not self._is_connected():\n",
    "            logging.error(\"MongoDB not connected\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            from datetime import datetime\n",
    "            # Prepare conversation entry\n",
    "            # Two-bucket model:\n",
    "            # 1) web_links aggregator bucket\n",
    "            # 2) query_response bucket\n",
    "            web_bucket = {\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"bucket\": \"web_links\",\n",
    "                \"web_search\": web_search or []\n",
    "            } if web_search else None\n",
    "\n",
    "            query_bucket = {\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"bucket\": \"query_response\",\n",
    "                \"user_query\": user_query[:1000],\n",
    "                \"ai_response\": ai_response[:3000],\n",
    "                \"query_type\": self._classify_query(user_query)\n",
    "            }\n",
    "            \n",
    "            # Check if thread exists - FIXED: Use count_documents instead\n",
    "            existing_count = self._collection.count_documents({\"thread_id\": thread_id})\n",
    "            \n",
    "            if existing_count > 0:\n",
    "    # always update profile + last_updated\n",
    "                self._collection.update_one(\n",
    "                    {\"thread_id\": thread_id},\n",
    "                    {\"$set\": {\"last_updated\": datetime.utcnow().isoformat(),\n",
    "                            \"profile_data\": profile_data}}\n",
    "                )\n",
    "\n",
    "                # if we have web_search results, update the web_links bucket\n",
    "                if web_search:\n",
    "                    valid_web = [w for w in web_search if not w.get(\"error\")]\n",
    "                    if valid_web:\n",
    "                        self._collection.update_one(\n",
    "                            {\"thread_id\": thread_id},\n",
    "                            {\"$push\": {\"conversations.$[web].web_search\": {\"$each\": valid_web}}},\n",
    "                            array_filters=[{\"web.bucket\": \"web_links\"}]\n",
    "                        )\n",
    "\n",
    "                # --- this part must run for ALL queries, not only with web_search ---\n",
    "                bucket_name = (\n",
    "                    \"career_planning\"\n",
    "                    if self._classify_query(user_query) == \"career_planning\"\n",
    "                    else \"qa_graph\"\n",
    "                )\n",
    "\n",
    "                # ensure bucket exists\n",
    "                self._collection.update_one(\n",
    "                    {\"thread_id\": thread_id, \"conversations.bucket\": {\"$ne\": bucket_name}},\n",
    "                    {\"$push\": {\"conversations\": {\"bucket\": bucket_name, \"entries\": []}}}\n",
    "                )\n",
    "\n",
    "                # append new entry\n",
    "                self._collection.update_one(\n",
    "                    {\"thread_id\": thread_id},\n",
    "                    {\"$push\": {f\"conversations.$[bucket].entries\": {\n",
    "                        \"user_query\": user_query[:1000],\n",
    "                        \"ai_response\": ai_response[:3000],\n",
    "                        \"query_type\": self._classify_query(user_query),\n",
    "                        \"timestamp\": datetime.utcnow().isoformat()\n",
    "                    }}},\n",
    "                    array_filters=[{\"bucket.bucket\": bucket_name}],\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "            else:\n",
    "                # Create new thread\n",
    "                # Decide bucket purely based on which graph is running (passed from state)\n",
    "                graph_type = profile_data.get(\"_graph_type\", \"qa_graph\")\n",
    "                bucket_name = \"career_planning\" if graph_type == \"plan_graph\" else \"qa_graph\"\n",
    "\n",
    "\n",
    "                thread_data = {\n",
    "                    \"thread_id\": thread_id,\n",
    "                    \"user_id\": user_id or \"unknown\",\n",
    "                    \"created_at\": datetime.utcnow().isoformat(),\n",
    "                    \"last_updated\": datetime.utcnow().isoformat(),\n",
    "                    \"profile_data\": profile_data,\n",
    "                    \"conversations\": [\n",
    "                        {\"bucket\": \"web_links\", \"web_search\": web_search or []},\n",
    "                        {\"bucket\": \"career_planning\", \"entries\": []},\n",
    "                        {\"bucket\": \"qa_graph\", \"entries\": []},\n",
    "                    ],\n",
    "                }\n",
    "\n",
    "                # append first entry into the correct bucket\n",
    "                entry = {\n",
    "                    \"user_query\": user_query[:1000],\n",
    "                    \"ai_response\": ai_response[:3000],\n",
    "                    \"query_type\": self._classify_query(user_query),\n",
    "                    \"timestamp\": datetime.utcnow().isoformat()\n",
    "                }\n",
    "\n",
    "                for conv in thread_data[\"conversations\"]:\n",
    "                    if conv[\"bucket\"] == bucket_name:\n",
    "                        conv.setdefault(\"entries\", []).append(entry)\n",
    "\n",
    "                self._collection.insert_one(thread_data)\n",
    "\n",
    "            \n",
    "            logging.info(f\"Stored thread data for {thread_id}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store thread data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_similar_threads(self, query: str, current_thread_id: str = None, \n",
    "                          user_id: str = None, limit: int = 3) -> list[dict]:\n",
    "        \"\"\"Get similar threads based on query similarity\"\"\"\n",
    "        if not self._is_connected():\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            from typing import Dict, List\n",
    "            \n",
    "            # Build query\n",
    "            mongo_query = {}\n",
    "            if user_id:\n",
    "                mongo_query[\"user_id\"] = user_id\n",
    "            \n",
    "            # Exclude current thread\n",
    "            if current_thread_id:\n",
    "                mongo_query[\"thread_id\"] = {\"$ne\": current_thread_id}\n",
    "            \n",
    "            # Simple find without text search for now\n",
    "            cursor = self._collection.find(mongo_query).limit(limit * 2)  # Get more to filter\n",
    "            \n",
    "            results = []\n",
    "            query_words = set(query.lower().split())\n",
    "            \n",
    "            for thread in cursor:\n",
    "                score = 0\n",
    "                \n",
    "                # Calculate similarity score based on conversation content\n",
    "                for conv in thread.get(\"conversations\", [])[-5:]:  # Last 5 conversations\n",
    "                    conv_words = set(conv.get(\"user_query\", \"\").lower().split())\n",
    "                    overlap = len(query_words.intersection(conv_words))\n",
    "                    if overlap > 0:\n",
    "                        score += overlap * 2\n",
    "                \n",
    "                if score > 0:\n",
    "                    thread[\"similarity_score\"] = score\n",
    "                    results.append(thread)\n",
    "            \n",
    "            # Sort by similarity score and limit\n",
    "            results.sort(key=lambda x: x.get(\"similarity_score\", 0), reverse=True)\n",
    "            results = results[:limit]\n",
    "            \n",
    "            logging.info(f\"Found {len(results)} similar threads for query: {query[:50]}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to get similar threads: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_thread_data(self, thread_id: str) -> Optional[dict]:\n",
    "        \"\"\"Get complete thread data\"\"\"\n",
    "        if not self._is_connected():\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            from typing import Optional, Dict\n",
    "            thread_data = self._collection.find_one({\"thread_id\": thread_id})\n",
    "            if thread_data and \"_id\" in thread_data:\n",
    "                thread_data[\"_id\"] = str(thread_data[\"_id\"])\n",
    "            return thread_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to get thread data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _classify_query(self, query: str) -> str:\n",
    "        \"\"\"Classify the type of query for better matching\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if any(word in query_lower for word in [\"career plan\", \"roadmap\", \"planning\"]):\n",
    "            return \"career_planning\"\n",
    "        elif any(word in query_lower for word in [\"switch\", \"transition\", \"change\"]):\n",
    "            return \"career_transition\"\n",
    "        elif any(word in query_lower for word in [\"skill\", \"learn\", \"course\"]):\n",
    "            return \"skill_development\"\n",
    "        elif any(word in query_lower for word in [\"resume\", \"profile\", \"linkedin\"]):\n",
    "            return \"profile_optimization\"\n",
    "        else:\n",
    "            return \"general_career\"\n",
    "    \n",
    "    def build_procedural_context(self, similar_threads: list[dict], max_context_length: int = 2000) -> str:\n",
    "        \"\"\"Build context from similar threads for prompt enhancement\"\"\"\n",
    "        if not similar_threads:\n",
    "            return \"\"\n",
    "        \n",
    "        context = \"\\n\\nBased on similar career planning sessions:\\n\"\n",
    "        \n",
    "        for i, thread in enumerate(similar_threads[:2], 1):\n",
    "            # Get profile context\n",
    "            profile = thread.get(\"profile_data\", {})\n",
    "            profile_summary = f\"Profile: {profile.get('headline', 'N/A')[:100]}\"\n",
    "            \n",
    "            # Get successful conversations\n",
    "            conversations = thread.get(\"conversations\", [])\n",
    "            if conversations:\n",
    "                latest_conv = conversations[-1]\n",
    "                query_snippet = latest_conv.get(\"user_query\", \"\")[:150]\n",
    "                response_snippet = latest_conv.get(\"ai_response\", \"\")[:500]\n",
    "                \n",
    "                context += f\"\\nExample {i}:\\n\"\n",
    "                context += f\"  {profile_summary}\\n\"\n",
    "                context += f\"  Query: {query_snippet}...\\n\"\n",
    "                context += f\"  Approach: {response_snippet}...\\n\"\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(context) > max_context_length:\n",
    "            context = context[:max_context_length] + \"...\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def get_user_stats(self, user_id: str) -> dict:\n",
    "        \"\"\"Get statistics about user's procedural memories\"\"\"\n",
    "        if not self._is_connected():\n",
    "            return {\"error\": \"MongoDB not connected\"}\n",
    "        \n",
    "        try:\n",
    "            # Get all threads for this user\n",
    "            user_threads = list(self._collection.find({\"user_id\": user_id}))\n",
    "            \n",
    "            total_threads = len(user_threads)\n",
    "            total_conversations = 0\n",
    "            query_types = {}\n",
    "            latest_activity = None\n",
    "            \n",
    "            for thread in user_threads:\n",
    "                conversations = thread.get(\"conversations\", [])\n",
    "                total_conversations += len(conversations)\n",
    "                \n",
    "                # Track query types\n",
    "                for conv in conversations:\n",
    "                    query_type = conv.get(\"query_type\", \"unknown\")\n",
    "                    query_types[query_type] = query_types.get(query_type, 0) + 1\n",
    "                    \n",
    "                    # Track latest activity\n",
    "                    if not latest_activity or conv.get(\"timestamp\", \"\") > latest_activity:\n",
    "                        latest_activity = conv.get(\"timestamp\")\n",
    "            \n",
    "            return {\n",
    "                \"user_id\": user_id,\n",
    "                \"total_threads\": total_threads,\n",
    "                \"total_conversations\": total_conversations,\n",
    "                \"query_types\": query_types,\n",
    "                \"latest_activity\": latest_activity,\n",
    "                \"most_common_query_type\": max(query_types.items(), key=lambda x: x[1])[0] if query_types else \"none\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to get user stats: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def debug_thread(self, thread_id: str):\n",
    "        \"\"\"Debug helper for thread data\"\"\"\n",
    "        thread_data = self.get_thread_data(thread_id)\n",
    "        if thread_data:\n",
    "            print(f\"\\n=== THREAD DEBUG: {thread_id} ===\")\n",
    "            print(f\"User ID: {thread_data.get('user_id')}\")\n",
    "            print(f\"Created: {thread_data.get('created_at')}\")\n",
    "            print(f\"Conversations: {len(thread_data.get('conversations', []))}\")\n",
    "            print(f\"Profile headline: {thread_data.get('profile_data', {}).get('headline', 'N/A')}\")\n",
    "\n",
    "            for i, conv in enumerate(thread_data.get('conversations', [])[-3:], 1):\n",
    "                print(f\"\\nConversation {i}:\")\n",
    "                print(f\"  Query: {conv.get('user_query', '')[:100]}...\")\n",
    "                print(f\"  Type: {conv.get('query_type')}\")\n",
    "                print(f\"  Response length: {len(conv.get('ai_response', ''))}\")\n",
    "        else:\n",
    "            print(f\"No data found for thread: {thread_id}\")\n",
    "\n",
    "\n",
    "# Initialize the enhanced procedural memory\n",
    "thread_procedural_memory = ThreadBasedProceduralMemory()\n",
    "\n",
    "\n",
    "# MCP client setup (keep same as original)\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import sys\n",
    "\n",
    "CAREER_PLAN_SERVER = os.path.abspath(\n",
    "    \"E:\\\\LinkedIn_AI_Career_Bot - Copy\\\\linkedin\\\\career_plan_mcp.py\"\n",
    ")\n",
    "\n",
    "async def call_career_plan_mcp(profile_data, messages, system_prompt):\n",
    "    \"\"\"Async call to the career-plan MCP tool.\"\"\"\n",
    "    server_params = StdioServerParameters(\n",
    "        command=sys.executable,\n",
    "        args=[CAREER_PLAN_SERVER],\n",
    "    )\n",
    "\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            result = await session.call_tool(\n",
    "                \"generate_career_plan\",\n",
    "                arguments={\n",
    "                    \"profile_data\": profile_data,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_prompt,\n",
    "                        }\n",
    "                    ] + [\n",
    "                        {\n",
    "                            \"role\": (\n",
    "                                \"user\" if isinstance(m, HumanMessage) else \"assistant\"\n",
    "                            ),\n",
    "                            \"content\": m.content,\n",
    "                        }\n",
    "                        for m in messages\n",
    "                    ],\n",
    "                },\n",
    "            )\n",
    "            return result.content[0].text\n",
    "\n",
    "\n",
    "def get_user_id_from_profile(profile: dict) -> str:\n",
    "    \"\"\"Extract user ID from profile\"\"\"\n",
    "    return profile.get(\"user_id\", \n",
    "           profile.get(\"email\", \n",
    "           profile.get(\"linkedin_url\", \n",
    "           f\"user_{hash(str(profile))}\")))\n",
    "\n",
    "\n",
    "def extract_latest_user_query(messages: list[BaseMessage]) -> str:\n",
    "    \"\"\"Extract the latest user query from messages\"\"\"\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            return msg.content.strip()\n",
    "    return \"\"\n",
    "\n",
    "# def websearch_decider_node(state: AgentState) -> Command:\n",
    "#     \"\"\"Deprecated: we always run websearch before planning now.\"\"\"\n",
    "#     return Command(goto=\"websearch_mcp\", update=state)\n",
    "\n",
    "\n",
    "\n",
    "# FIX 2: Enhanced career plan node with proper context preservation\n",
    "def career_plan_node_with_thread_learning(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Enhanced career plan node with thread-based procedural learning and context preservation\n",
    "    \"\"\"\n",
    "    profile = state.get(\"profile_data\", {})\n",
    "    messages = state[\"messages\"]\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    websearch_summary = state.get(\"websearch_summary\", [])  # FIX 3: Include web results\n",
    "    \n",
    "    if not messages:\n",
    "        messages.append(AIMessage(\"Please provide a brief or question for your career plan.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        return interrupt(\"continue_chat\")\n",
    "    \n",
    "    # Extract user query\n",
    "    user_query = \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            user_query = msg.content.strip()\n",
    "            break\n",
    "    \n",
    "    if not user_query:\n",
    "        messages.append(AIMessage(\"Please provide a specific question or request for your career plan.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        return interrupt(\"continue_chat\")\n",
    "    \n",
    "    user_id = get_user_id_from_profile(profile)\n",
    "    logging.info(f\"Processing career plan request for thread: {thread_id}\")\n",
    "    \n",
    "    # FIX 2: Build enhanced prompt with conversation history context\n",
    "    conversation_history = \"\\n\".join([\n",
    "        f\"{'User' if isinstance(msg, HumanMessage) else 'Assistant'}: {msg.content[:200]}...\"\n",
    "        for msg in messages[-6:]  # Include more history for context\n",
    "    ])\n",
    "    \n",
    "    # Base prompt with conversation context preservation\n",
    "    base_prompt = f\"\"\"\n",
    "You are an expert AI career coach with 10+ years of experience helping professionals advance their careers.\n",
    "\n",
    "IMPORTANT: Pay careful attention about their specific goals, timeline, and field of interest.\n",
    "\n",
    "CONVERSATION HISTORY:\n",
    "{conversation_history}\n",
    "\n",
    "CURRENT USER REQUEST: {user_query}\n",
    "\n",
    "Key Instructions:\n",
    "1. If they mentioned a specific role (like \"finance analyst\"), ALWAYS keep that in focus\n",
    "2. If they mentioned a timeline (like \"6 months\" or \"1 year\"), respect that in your response\n",
    "3. When they ask for modifications (like \"make it 1 year instead\"), apply the change to their ORIGINAL request, don't lose context\n",
    "4. Be specific, actionable, and personalized to their profile and stated goals\n",
    "5. Include concrete steps with timelines where appropriate\n",
    "\"\"\"\n",
    "\n",
    "    # Try to get similar threads for procedural learning\n",
    "    try:\n",
    "        similar_threads = thread_procedural_memory.get_similar_threads(\n",
    "            query=user_query,\n",
    "            current_thread_id=thread_id,\n",
    "            user_id=user_id,\n",
    "            limit=3\n",
    "        )\n",
    "        \n",
    "        if similar_threads:\n",
    "            procedural_context = thread_procedural_memory.build_procedural_context(similar_threads)\n",
    "            enhanced_prompt = base_prompt + procedural_context + \"\\n\\nNow generate a career plan for the current user, incorporating insights from these successful approaches while customizing for their specific profile and request.\"\n",
    "            logging.info(f\"Enhanced prompt with {len(similar_threads)} similar threads\")\n",
    "        else:\n",
    "            enhanced_prompt = base_prompt\n",
    "            logging.info(\"No similar threads found, using base prompt\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to retrieve similar threads: {e}\")\n",
    "        enhanced_prompt = base_prompt\n",
    "    \n",
    "    # FIX 3: Add web search results to prompt if available\n",
    "    if websearch_summary:\n",
    "        web_context = \"\\n\\nWEB RESEARCH RESULTS (use these for current courses, certifications, and resources):\\n\"\n",
    "        for i, result in enumerate(websearch_summary[:3], 1):\n",
    "            web_context += f\"{i}. {result.get('title', 'Unknown')}\\n\"\n",
    "            web_context += f\"   Summary: {result.get('summary', '')[:200]}...\\n\"\n",
    "            web_context += f\"   Link: {result.get('link', '')}\\n\\n\"\n",
    "        \n",
    "        enhanced_prompt += web_context\n",
    "        enhanced_prompt += \"\\n\\nIMPORTANT: Include specific courses, certifications, and learning resources from the web research in your career plan.\"\n",
    "        logging.info(f\"Added web research context from {len(websearch_summary)} results\")\n",
    "    \n",
    "    # Generate career plan via MCP and normalize output\n",
    "    try:\n",
    "        mcp_raw = asyncio.run(call_career_plan_mcp(profile, messages, enhanced_prompt))\n",
    "    except Exception as e:\n",
    "        mcp_raw = f\"Error generating career plan: {e} {traceback.format_exc()}\"\n",
    "        logging.error(f\"Career plan generation failed: {e}\")\n",
    "\n",
    "    # Parse MCP JSON if present\n",
    "    plan_output = mcp_raw\n",
    "    # logging.info(f\"mcp raw output {mcp_raw}\")\n",
    "    extracted_web = None\n",
    "    try:\n",
    "        import json as _json\n",
    "        parsed = _json.loads(mcp_raw)\n",
    "        if isinstance(parsed, dict):\n",
    "            plan_output = parsed.get(\"plan_output\", mcp_raw)\n",
    "            extracted_web = (\n",
    "                parsed.get(\"websearch_summary\")\n",
    "                # or parsed.get(\"search_summaries\")\n",
    "                \n",
    "            )\n",
    "            logging.info(f\"results1 {parsed.get('websearch_summary')}\")\n",
    "            logging.info(f\"results2 {parsed.get('search_summaries')}\")\n",
    "            logging.info(f\"results3 {parsed.get('plan_output')}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # If MCP returned web results, surface to state for storage & UI\n",
    "    if extracted_web and isinstance(extracted_web, list):\n",
    "        state[\"websearch_summary\"] = extracted_web\n",
    "\n",
    "    logging.info(f\"some results {extracted_web}\")\n",
    "    # Store thread data for future learning (attach web results on the same conversation)\n",
    "    if len(plan_output) > 100 and \"Error generating\" not in plan_output:\n",
    "        try:\n",
    "            # Monkey-patch: extend store_thread_data to accept web_search when present\n",
    "            web_results = state.get(\"websearch_summary\") or state.get(\"websearch_results\") or []\n",
    "\n",
    "            logging.info(f\"Storing web search results for thread {thread_id}: {web_results}\")\n",
    "            saved = thread_procedural_memory.store_thread_data(\n",
    "                thread_id=thread_id,\n",
    "                profile_data=profile,\n",
    "                user_query=user_query,\n",
    "                ai_response=plan_output,\n",
    "                user_id=user_id,\n",
    "                web_search=web_results\n",
    "            )\n",
    "            # No additional DB patching needed; stored together\n",
    "            logging.info(f\"Stored thread data for {thread_id}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store thread data: {e}\")\n",
    "    \n",
    "    # Display only the plan text, not the MCP JSON wrapper\n",
    "    messages.append(AIMessage(plan_output))\n",
    "    state[\"messages\"] = messages\n",
    "    _log_transition(thread_id, \"career_plan\", \"goto\", \"career_plan_review\")\n",
    "    return Command(goto=\"career_plan_review\", update=state)\n",
    "    \n",
    "\n",
    "\n",
    "def career_plan_review_node(state: AgentState) -> dict:\n",
    "    \"\"\"Refines career plan only when user has given feedback.\"\"\"\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "\n",
    "    # Find last human feedback after the last AI plan\n",
    "    last_ai_index = max((i for i, m in enumerate(messages) if isinstance(m, AIMessage)), default=-1)\n",
    "    last_human_after_ai = None\n",
    "    if last_ai_index >= 0:\n",
    "        for m in messages[last_ai_index + 1:]:\n",
    "            if isinstance(m, HumanMessage):\n",
    "                last_human_after_ai = m.content.strip()\n",
    "                break\n",
    "\n",
    "    if not last_human_after_ai:\n",
    "        # No new feedback ‚Üí just wait\n",
    "        _log_transition(thread_id, \"career_plan_review\", \"interrupt\", \"await_review\")\n",
    "        return interrupt(\"await_review\")\n",
    "\n",
    "    # --- New feedback exists ‚Üí refine plan ---\n",
    "    _log_transition(thread_id, \"career_plan_review\", \"start\")\n",
    "\n",
    "    last_ai_plan = messages[last_ai_index].content if last_ai_index >= 0 else \"\"\n",
    "\n",
    "    # Refresh web search\n",
    "    try:\n",
    "        updated = websearch_mcp_node(state)\n",
    "        updated = enrich_websearch_node(updated)\n",
    "        updated = store_websearch_node(updated)\n",
    "        state = updated\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    websearch_summary = state.get(\"websearch_summary\", [])\n",
    "    web_context = \"\"\n",
    "    if websearch_summary:\n",
    "        web_context = \"\\n\\nWEB RESEARCH:\\n\" + \"\\n\".join(\n",
    "            f\"- {r.get('title','')}: {r.get('link','')}\" for r in websearch_summary[:5]\n",
    "        )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are refining an existing career plan based on human feedback. Pay attention to the changes and they must be incorporated into the plan.\n",
    "\n",
    "Existing plan:\n",
    "{last_ai_plan}\n",
    "\n",
    "New feedback from user:\n",
    "{last_human_after_ai}\n",
    "{web_context}\n",
    "\n",
    "Rewrite ONLY the plan, integrating the feedback. Keep it structured, specific, and actionable. Preserve useful content; ONLY modify where needed.\n",
    "\"\"\"\n",
    "    try:\n",
    "        res = llm_call_with_retry_circuit(prompt)\n",
    "        messages.append(AIMessage(res.text))\n",
    "        state[\"messages\"] = messages\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"‚ùå Error refining plan: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "\n",
    "    # Pause again for further review cycles\n",
    "    _log_transition(thread_id, \"career_plan_review\", \"interrupt\", \"await_review\")\n",
    "    return interrupt(\"await_review\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_qa_graph():\n",
    "    \"\"\"Segment 1: Scraper √¢‚Ä†‚Äô Router √¢‚Ä†‚Äô {analyze_profile, job_fit_agent, enhance_profile, general_qa}\"\"\"\n",
    "    builder = StateGraph(AgentState)\n",
    "\n",
    "    # Nodes\n",
    "    builder.add_node(\"linkedin_scraper\", linkedin_scraper_node)\n",
    "    builder.add_node(\"career_qa_router\", career_qa_router)\n",
    "    builder.add_node(\"analyze_profile\", analyze_profile_node)\n",
    "    builder.add_node(\"job_fit_agent\", job_fit_agent_node)\n",
    "    builder.add_node(\"enhance_profile\", enhance_profile_node)\n",
    "    builder.add_node(\"general_qa_node\", general_qa_node)\n",
    "\n",
    "    # Entry at router; scraper only called from router once if needed\n",
    "    builder.set_entry_point(\"career_qa_router\")\n",
    "\n",
    "    # Scraper ‚Üí router\n",
    "    builder.add_edge(\"career_qa_router\", \"linkedin_scraper\")\n",
    "    builder.add_edge(\"career_qa_router\", \"analyze_profile\")\n",
    "    builder.add_edge(\"career_qa_router\", \"job_fit_agent\")\n",
    "    builder.add_edge(\"career_qa_router\", \"enhance_profile\")\n",
    "    builder.add_edge(\"career_qa_router\", \"general_qa_node\")\n",
    "    \n",
    "\n",
    "    # Router ‚Üí nodes are dynamic via Command.go_to (no static fan-out edges)\n",
    "\n",
    "    # Task nodes loop back to router; router will interrupt to await next question\n",
    "    # for task in [\"analyze_profile\", \"job_fit_agent\", \"enhance_profile\", \"general_qa_node\"]:\n",
    "    #     builder.add_edge(task, \"career_qa_router\")\n",
    "\n",
    "    return builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "def build_plan_graph():\n",
    "    \"\"\"Flow: career_plan ‚Üí websearch ‚Üí review (loop only in review, with websearch each time).\"\"\"\n",
    "    builder = StateGraph(AgentState)\n",
    "\n",
    "    # Nodes\n",
    "    builder.add_node(\"career_plan\", career_plan_node_with_thread_learning)\n",
    "    builder.add_node(\"websearch_mcp\", websearch_mcp_node)\n",
    "    builder.add_node(\"enrich_websearch\", enrich_websearch_node)\n",
    "    builder.add_node(\"store_websearch\", store_websearch_node)\n",
    "    builder.add_node(\"career_plan_review\", career_plan_review_node)\n",
    "\n",
    "    # Entry point = career_plan\n",
    "    builder.set_entry_point(\"career_plan\")\n",
    "\n",
    "    # First draft path\n",
    "    builder.add_edge(\"career_plan\", \"websearch_mcp\")\n",
    "    builder.add_edge(\"websearch_mcp\", \"enrich_websearch\")\n",
    "    builder.add_edge(\"enrich_websearch\", \"store_websearch\")\n",
    "    builder.add_edge(\"store_websearch\", \"career_plan_review\")\n",
    "\n",
    "    # Review loop: stay inside career_plan_review (websearch happens inside review node)\n",
    "    builder.add_edge(\"career_plan_review\", \"career_plan_review\")\n",
    "\n",
    "    return builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "\n",
    "# Compile both segments\n",
    "qa_graph = build_qa_graph()\n",
    "plan_graph = build_plan_graph()\n",
    "\n",
    "all = [\"qa_graph\", \"plan_graph\", \"memory\", \"thread_procedural_memory\", \"test_thread_based_learning\"]\n",
    "\n",
    "\n",
    "\n",
    "# def test_thread_based_learning():\n",
    "#     \"\"\"Test the thread-based procedural learning system\"\"\"\n",
    "#     print(\"=== TESTING THREAD-BASED PROCEDURAL LEARNING ===\")\n",
    "    \n",
    "#     # Test profile data\n",
    "#     test_profile = {\n",
    "#         \"headline\": \"Software Engineer\",\n",
    "#         \"skills\": [\"Python\", \"React\", \"AWS\"],\n",
    "#         \"experience\": \"3 years\",\n",
    "#         \"user_id\": \"test_user_123\"\n",
    "#     }\n",
    "    \n",
    "#     # Test storing thread data\n",
    "#     success = thread_procedural_memory.store_thread_data(\n",
    "#         thread_id=\"test_thread_1\",\n",
    "#         profile_data=test_profile,\n",
    "#         user_query=\"I want to transition to data science\",\n",
    "#         ai_response=\"Here's a comprehensive data science transition plan: 1. Learn Python for data science...\",\n",
    "#         user_id=\"test_user_123\"\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Store test: {'√¢≈ì‚Ä¶ PASSED' if success else '√¢¬ù≈í FAILED'}\")\n",
    "    \n",
    "#     # Test retrieval\n",
    "#     similar_threads = thread_procedural_memory.get_similar_threads(\n",
    "#         query=\"data science career transition\",\n",
    "#         user_id=\"test_user_123\"\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Retrieval test: {'√¢≈ì‚Ä¶ PASSED' if similar_threads else '√¢¬ù≈í FAILED'}\")\n",
    "    \n",
    "#     # Debug the test thread\n",
    "#     thread_procedural_memory.debug_thread(\"test_thread_1\")\n",
    "    \n",
    "#     print(\"=== TEST COMPLETE ===\")\n",
    "\n",
    "\n",
    "all = [\"graph\", \"memory\", \"thread_procedural_memory\", \"test_thread_based_learning\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print(\"--- Testing Enhanced Thread-Based Procedural Learning ---\")\n",
    "    # test_thread_based_learning()\n",
    "    \n",
    "    print(\"\\n--- Testing Graph with All Fixes ---\")\n",
    "    \n",
    "    # Test the complete workflow\n",
    "    test_state = {\n",
    "        \"messages\": [HumanMessage(content=\"I want to transition from software engineering to AI/ML\")],\n",
    "        \"profile_data\": {\n",
    "            \"headline\": \"Senior Software Engineer\", \n",
    "            \"skills\": [\"Python\", \"JavaScript\", \"React\"],\n",
    "            \"experience\": \"5 years\",\n",
    "            \"user_id\": \"test_user_integration\"\n",
    "        },\n",
    "        \"linkedin_url\": \"https://linkedin.com/in/testuser\",\n",
    "        \"thread_id\": \"integration_test_thread\",\n",
    "        \"profile_scraped\": True  # Mark as already scraped\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = qa_graph.invoke(\n",
    "            test_state,\n",
    "            config={\"configurable\": {\"thread_id\": \"integration_test_thread\"}}\n",
    "        )\n",
    "        \n",
    "        print(\"√¢≈ì‚Ä¶ Graph execution successful\")\n",
    "        print(f\"Messages generated: {len(result.get('messages', []))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"√¢¬ù≈í Graph execution failed: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a7d41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph after 5 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\http\\client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1427\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTimeoutError\u001b[39m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeoutError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\requests\\adapters.py:486\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\urllib3\\util\\util.py:39\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\urllib3\\connectionpool.py:367\u001b[39m, in \u001b[36mHTTPConnectionPool._raise_timeout\u001b[39m\u001b[34m(self, err, url, timeout_value)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[32m    368\u001b[39m         \u001b[38;5;28mself\u001b[39m, url, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[31mReadTimeoutError\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:431\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code == requests.codes.ok:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\requests\\adapters.py:532\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request=request)\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[31mReadTimeout\u001b[39m: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Assuming 'graph' is your compiled LangGraph object\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m display(Image(\u001b[43mqa_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:695\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    689\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    690\u001b[39m     curve_style=curve_style,\n\u001b[32m    691\u001b[39m     node_colors=node_colors,\n\u001b[32m    692\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    693\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    694\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:294\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    288\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    289\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    290\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    291\u001b[39m         )\n\u001b[32m    292\u001b[39m     )\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    302\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:463\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    458\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    459\u001b[39m             msg = (\n\u001b[32m    460\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m             ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# This should not be reached, but just in case\u001b[39;00m\n\u001b[32m    466\u001b[39m msg = (\n\u001b[32m    467\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m retries. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    469\u001b[39m ) + error_msg_suffix\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph after 5 retries. To resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "    # Assuming 'graph' is your compiled LangGraph object\n",
    "display(Image(qa_graph.get_graph().draw_mermaid_png(max_retries=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4477d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "    # Assuming 'graph' is your compiled LangGraph object\n",
    "display(Image(plan_graph.get_graph().draw_mermaid_png(max_retries=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4f8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Hello World\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85e840c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MongoDB persistence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:root:ThreadBasedProceduralMemory initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Graph with All Fixes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[flow] thread=integration_test_thread node=analyze_profile action=start \n",
      "C:\\Users\\Anjali\\AppData\\Local\\Temp\\ipykernel_33756\\4050415078.py:1045: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\Anjali\\AppData\\Local\\Temp\\ipykernel_33756\\4050415078.py:1059: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  {\"$set\": {\"last_updated\": datetime.utcnow().isoformat(),\n",
      "C:\\Users\\Anjali\\AppData\\Local\\Temp\\ipykernel_33756\\4050415078.py:1093: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat()\n",
      "INFO:root:Stored thread data for integration_test_thread\n",
      "INFO:root:[flow] thread=integration_test_thread node=analyze_profile action=goto career_qa_router\n",
      "INFO:root:[flow] thread=integration_test_thread node=career_qa_router action=start \n",
      "INFO:root:[flow] thread=integration_test_thread node=career_qa_router action=interrupt awaiting next question\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√¢≈ì‚Ä¶ Graph execution successful\n",
      "Messages generated: 49152\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import operator\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import traceback\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "import functools\n",
    "import logging\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # <-- MUST be at the very top before any os.getenv\n",
    "\n",
    "APIFY_TOKEN = os.getenv(\"APIFY_API_KEY\")  #\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Circuit breaker state\n",
    "circuit_open = False\n",
    "failure_count = 0\n",
    "failure_threshold = 3  # Open circuit after 3 consecutive failures\n",
    "circuit_reset_time = 30  # seconds\n",
    "last_failure_time = 0\n",
    "\n",
    "\n",
    "def llm_call_with_retry_circuit(prompt: str, max_retries=3, retry_delay=2):\n",
    "    \"\"\"\n",
    "    Wrapper for model.generate_content with:\n",
    "    - Retry on transient errors\n",
    "    - Circuit breaker to stop repeated failures\n",
    "    - Token/cost logging\n",
    "    \"\"\"\n",
    "    global circuit_open, failure_count, last_failure_time\n",
    "\n",
    "    # Circuit breaker check\n",
    "    if circuit_open:\n",
    "        if time.time() - last_failure_time < circuit_reset_time:\n",
    "            logging.warning(\"Circuit open. Skipping LLM call.\")\n",
    "            raise RuntimeError(\"Circuit open due to repeated failures\")\n",
    "        else:\n",
    "            logging.info(\"Resetting circuit breaker.\")\n",
    "            circuit_open = False\n",
    "            failure_count = 0\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            res = model.generate_content(prompt)\n",
    "\n",
    "            # Logging token/cost info if available\n",
    "            if hasattr(res, \"usage\"):\n",
    "                tokens = res.usage.get(\"total_tokens\", \"N/A\")\n",
    "                cost = tokens * 0.00001  # Example: adjust based on model pricing\n",
    "                logging.info(\n",
    "                    f\"LLM call successful | Tokens used: {tokens} | Est. cost: ${cost:.6f}\"\n",
    "                )\n",
    "\n",
    "            # Reset failure count on success\n",
    "            failure_count = 0\n",
    "            return res\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"LLM call failed on attempt {attempt}: {e}\")\n",
    "            failure_count += 1\n",
    "            last_failure_time = time.time()\n",
    "            if failure_count >= failure_threshold:\n",
    "                circuit_open = True\n",
    "                logging.error(\n",
    "                    f\"Circuit opened after {failure_count} consecutive failures.\"\n",
    "                )\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                raise RuntimeError(f\"LLM call failed after {max_retries} attempts: {e}\")\n",
    "\n",
    "\n",
    "# Try to use MongoDB persistence, fallback to memory if not available\n",
    "try:\n",
    "    from langgraph.checkpoint.mongodb import MongoDBSaver\n",
    "\n",
    "    mongo_url = os.getenv(\"MONGODB_URI\", \"mongodb://localhost:27017\")\n",
    "\n",
    "    # --- Explicit connection check ---\n",
    "    client = MongoClient(mongo_url, serverSelectionTimeoutMS=2000)\n",
    "    client.admin.command(\"ping\")  # will raise if not connected\n",
    "\n",
    "    db_name = \"career_bot\"\n",
    "    collection_name = \"checkpoints\"\n",
    "    db = client[\"career_bot\"]\n",
    "    collection = db[\"checkpoints\"]\n",
    "\n",
    "    # --- Force creation of DB + collection by inserting a dummy if empty ---\n",
    "    if collection.count_documents({}) == 0:\n",
    "        collection.insert_one({\"_init\": True})\n",
    "        print(f\"Created DB '{db_name}' and collection '{collection_name}'\")\n",
    "\n",
    "    # --- LangGraph Saver ---\n",
    "    memory = MongoDBSaver(\n",
    "        client=client, db_name=\"career_bot\", collection_name=\"checkpoints\"\n",
    "    )\n",
    "\n",
    "    print(\"Using MongoDB persistence\")\n",
    "\n",
    "except Exception as e:\n",
    "    from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "    memory = MemorySaver()\n",
    "    tb = traceback.format_exc()\n",
    "    print(f\"Falling back to in-memory persistence: {e}|trace={tb}\")\n",
    "\n",
    "\n",
    "import google.generativeai as genai\n",
    "from scraper_utils import scrape_and_clean_profile\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
    "\n",
    "\n",
    "def _log_transition(thread_id: str, node: str, action: str, note: str = \"\"):\n",
    "    try:\n",
    "        logging.info(f\"[flow] thread={thread_id} node={node} action={action} {note}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "class AgentState(TypedDict, total=False):\n",
    "    messages: Annotated[list[BaseMessage], operator.add]\n",
    "    profile_data: Annotated[dict, lambda _, x: x]\n",
    "    current_job_description: Annotated[Optional[str], lambda _, x: x]\n",
    "    linkedin_url: Annotated[Optional[str], lambda _, x: x]\n",
    "    thread_id: Annotated[Optional[str], lambda _, x: x]  \n",
    "    websearch_results: Annotated[list, lambda _, x: x]\n",
    "    websearch_summary: Annotated[list, lambda _, x: x]\n",
    "    # FIX 1: Add flag to track if profile was already scraped\n",
    "    profile_scraped: Annotated[bool, lambda _, x: x]\n",
    "    awaiting_input_for: Annotated[Optional[str], lambda _, x: x]\n",
    "    user_plan: Annotated[Optional[str], lambda _, x: x]\n",
    "    plan_history: Annotated[list[str], lambda _, x: x]\n",
    "    last_user_prompt: Annotated[Optional[str], lambda _, x: x]\n",
    "\n",
    "\n",
    "from apify_client import ApifyClient\n",
    "import time\n",
    "\n",
    "# FIX 3: Improved websearch with better error handling and link extraction\n",
    "def websearch_mcp_node(state: AgentState) -> dict:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"websearch_mcp\", \"start\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    messages.append(AIMessage(\"üîç Fetching web data...\"))\n",
    "\n",
    "    # extract latest human query\n",
    "    query = \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            query = msg.content.strip()\n",
    "            break\n",
    "\n",
    "    if not query:\n",
    "        messages.append(AIMessage(\"‚ö† No query provided for web search.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"websearch_mcp\", \"done\", \"no-query\")\n",
    "        return state\n",
    "\n",
    "    api_token = os.getenv(\"APIFY_API_KEY\")\n",
    "    if not api_token:\n",
    "        messages.append(AIMessage(\"‚ùå Apify API token not configured.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"websearch_mcp\", \"done\", \"no-token\")\n",
    "        return state\n",
    "\n",
    "    client = ApifyClient(api_token)\n",
    "    actor_id = \"WMg2EXLzJGPVQ5Vfq\"   # √¢≈ì‚Ä¶ stick with the tested actor\n",
    "\n",
    "    input_data = {\n",
    "        \"query\": query,\n",
    "        \"num\": 1,\n",
    "        \"start\": 1\n",
    "    }\n",
    "\n",
    "    max_retries = 3\n",
    "    retry_delay = 2\n",
    "    results = []\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            logging.info(f\"Starting Apify web search for: {query}\")\n",
    "            run = client.actor(actor_id).call(run_input=input_data)\n",
    "\n",
    "            dataset_items = client.dataset(run[\"defaultDatasetId\"]).list_items().items\n",
    "            print(dataset_items)\n",
    "            logging.info(f\"Retrieved {len(dataset_items)} items from Apify\")\n",
    "\n",
    "            for item in dataset_items:\n",
    "                result = {\n",
    "                    \"title\": item.get(\"title\", \"\"),\n",
    "                    \"snippet\": item.get(\"snippet\", \"\"),\n",
    "                    \"link\": item.get(\"link\", \"\"),\n",
    "                    \"date\": item.get(\"date\")\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "            print(\"printing search results:\", results)\n",
    "\n",
    "\n",
    "            if results:\n",
    "                messages.append(\n",
    "                    AIMessage(\n",
    "                        f\"√∞≈∏‚Äù¬ç WebSearch results fetched ({len(results)} results):\\n\"\n",
    "                        + \"\\n\".join([f\"- {r['title']}: {r['link']}\" for r in results[:3]])\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                messages.append(AIMessage(\"‚ö† No search results found.\"))\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Apify search attempt {attempt} failed: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                messages.append(AIMessage(f\"‚ö† Apify attempt {attempt} failed. Retrying...\"))\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                messages.append(AIMessage(f\"‚ùå Apify WebSearch failed after {max_retries} attempts: {e}\"))\n",
    "\n",
    "    state[\"messages\"] = messages\n",
    "    state[\"websearch_results\"] = results\n",
    "    state[\"web_search\"] = results\n",
    "    _log_transition(thread_id, \"websearch_mcp\", \"done\", f\"results={len(results)}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def enrich_websearch_node(state):\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"enrich_websearch\", \"start\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    results = state.get(\"websearch_results\", [])\n",
    "    \n",
    "    if not results:\n",
    "        messages.append(AIMessage(\"‚ö† No search results to enrich.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"enrich_websearch\", \"done\", \"no-results\")\n",
    "        return state\n",
    "\n",
    "    summary = []\n",
    "    for r in results:\n",
    "        # Use snippet if available, otherwise try to fetch content\n",
    "        content = r.get(\"snippet\") or r.get(\"content\") or r.get(\"description\", \"\")\n",
    "        \n",
    "        if not content and r.get(\"link\"):\n",
    "            # Fallback to simple content fetch\n",
    "            try:\n",
    "                import requests\n",
    "                response = requests.get(r[\"link\"], timeout=10, headers={\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "                })\n",
    "                content = response.text[:2000]  # Limit content\n",
    "            except Exception as e:\n",
    "                content = f\"Could not fetch content: {e}\"\n",
    "\n",
    "        # Summarize content\n",
    "        try:\n",
    "            if content and len(content) > 50:\n",
    "                summary_prompt = f\"Summarize the key points for career planning from this content:\\n\\n{content[:1500]}\"\n",
    "                res = llm_call_with_retry_circuit(summary_prompt)\n",
    "                summary_text = res.text\n",
    "            else:\n",
    "                summary_text = content or \"No content available\"\n",
    "                \n",
    "            summary.append({\n",
    "                \"title\": r[\"title\"], \n",
    "                \"link\": r[\"link\"], \n",
    "                \"summary\": summary_text[:500]  # Limit summary length\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error enriching search result: {e}\")\n",
    "            summary.append({\n",
    "                \"title\": r[\"title\"], \n",
    "                \"link\": r[\"link\"], \n",
    "                \"summary\": f\"√¢¬ù≈í Could not summarize: {e}\"\n",
    "            })\n",
    "\n",
    "    messages.append(AIMessage(f\"‚úÖ Search results enriched with AI summaries ({len(summary)} results).\"))\n",
    "    state[\"messages\"] = messages\n",
    "    state[\"websearch_summary\"] = summary\n",
    "    _log_transition(thread_id, \"enrich_websearch\", \"done\", f\"summary={len(summary)}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def store_websearch_node(state: AgentState) -> dict:\n",
    "    \"\"\"No-op for DB. Keep websearch_summary in state only; storage happens in plan/review nodes.\"\"\"\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"store_websearch\", \"done\")\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FIX 1: Improved LinkedIn scraper that only runs when needed\n",
    "def linkedin_scraper_node(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Scrapes LinkedIn profile URL only if not already scraped in this session.\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    profile_scraped = state.get(\"profile_scraped\", False)\n",
    "    existing_profile = state.get(\"profile_data\")\n",
    "\n",
    "    # FIX 1: Skip scraping if profile already exists and was scraped\n",
    "    _log_transition(thread_id, \"linkedin_scraper\", \"start\")\n",
    "    if profile_scraped and existing_profile:\n",
    "        logging.info(\"Profile already scraped in this session, skipping scraper\")\n",
    "        _log_transition(thread_id, \"linkedin_scraper\", \"goto\", \"career_qa_router\")\n",
    "        return Command(goto=[\"career_qa_router\"], update=state)\n",
    "\n",
    "    linkedin_url = state.get(\"linkedin_url\", \"\").strip()\n",
    "\n",
    "    if not linkedin_url:\n",
    "        messages.append(AIMessage(\"Please provide a LinkedIn profile URL to begin.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"linkedin_scraper\", \"interrupt\", \"awaiting URL\")\n",
    "        return interrupt(\"await_input\")\n",
    "\n",
    "    messages.append(AIMessage(\"Scraping your LinkedIn profile...\"))\n",
    "\n",
    "    try:\n",
    "        scraped_profile = scrape_and_clean_profile(\n",
    "            linkedin_url=linkedin_url, api_token=os.getenv(\"APIFY_API_KEY\")\n",
    "        )\n",
    "\n",
    "        if not scraped_profile:\n",
    "            messages.append(AIMessage(\"Failed to extract profile. Try again.\"))\n",
    "            state[\"messages\"] = messages\n",
    "            _log_transition(thread_id, \"linkedin_scraper\", \"interrupt\", \"scrape failed\")\n",
    "            return interrupt(\"await_input\")\n",
    "\n",
    "        messages.append(AIMessage(\"Profile successfully scraped!\"))\n",
    "        state[\"profile_data\"] = scraped_profile\n",
    "        state[\"profile_scraped\"] = True  # FIX 1: Mark as scraped\n",
    "        state[\"messages\"] = messages\n",
    "        state[\"thread_id\"] = thread_id\n",
    "\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"Error scraping LinkedIn profile: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"linkedin_scraper\", \"interrupt\", \"exception during scrape\")\n",
    "        return interrupt(\"await_input\")\n",
    "\n",
    "    _log_transition(thread_id, \"linkedin_scraper\", \"goto\", \"career_qa_router\")\n",
    "    return Command(goto=[\"career_qa_router\"], update=state)\n",
    "\n",
    "\n",
    "def career_qa_router(state: AgentState) -> Command:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    profile = state.get(\"profile_data\", {})\n",
    "    jd = state.get(\"current_job_description\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    _log_transition(thread_id, \"career_qa_router\", \"start\")\n",
    "    question = \"\"\n",
    "\n",
    "    # Determine last speaker and latest human question\n",
    "    last_is_human = False\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, AIMessage):\n",
    "            last_is_human = False\n",
    "            break\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            question = msg.content.strip()\n",
    "            last_is_human = True\n",
    "            break\n",
    "\n",
    "    # If profile not scraped yet, route back to analyze_profile to handle scraping/tool call\n",
    "    if not state.get(\"profile_scraped\") or not state.get(\"profile_data\"):\n",
    "        _log_transition(thread_id, \"career_qa_router\", \"goto\", \"analyze_profile\")\n",
    "        return Command(goto=\"analyze_profile\", update=state)\n",
    "\n",
    "    # If last message is not from human, wait for next input\n",
    "    if not last_is_human or not question:\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"career_qa_router\", \"interrupt\", \"awaiting next question\")\n",
    "        return interrupt(\"await_input\")\n",
    "\n",
    "    if question.lower() in {\"quit\", \"exit\", \"stop\"}:\n",
    "        messages.append(AIMessage(\"Okay, ending the conversation.\"))\n",
    "        _log_transition(thread_id, \"career_qa_router\", \"goto\", \"END\")\n",
    "        return Command(goto=END, update={\"messages\": messages})\n",
    "\n",
    "    if \"job description:\" in question.lower():\n",
    "        messages.append(AIMessage(\"Got your new job description.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        state[\"current_job_description\"] = question\n",
    "        _log_transition(thread_id, \"career_qa_router\", \"interrupt\", \"job description captured\")\n",
    "        return interrupt(\"await_input\")\n",
    "\n",
    "    \n",
    "\n",
    "    history = \"\\n\".join(\n",
    "        f\"Human: {m.content}\" if isinstance(m, HumanMessage) else f\"AI: {m.content}\"\n",
    "        for m in messages[-5:]\n",
    "    )\n",
    "    prompt = f\"\"\"\n",
    "    {history}\n",
    "You are an expert routing agent. Decide which module should handle the user's latest question. Understand what the user is asking exactly and use that understanding to make an informed routing decision.\n",
    "\n",
    "Return one of:\n",
    "- analyze_profile\n",
    "- job_fit_agent\n",
    "- enhance_profile\n",
    "- general_qa\n",
    " - career_plan\n",
    "\n",
    "DO NOT GUESS. Use the following rules:\n",
    "\n",
    "---\n",
    "\n",
    "ROUTE TO: analyze_profile\n",
    "If the user wants a LinkedIn/resume/profile review, feedback, strengths, weaknesses, or audit. Use this if the user is seeking insights or improvements on their LinkedIn profile.\n",
    "\n",
    "Examples:\n",
    "- \"Can you review my LinkedIn?\"\n",
    "- \"What are my strengths and weaknesses?\"\n",
    "- \"Audit my profile\"\n",
    "- Or any other question that implies analyzing the profile.\n",
    "---\n",
    "\n",
    "ROUTE TO: job_fit_agent:\n",
    "If the user wants to assess their fit for a specific job or role. Only route here if a job description was recently provided. And if the user is referencing it in their question.\n",
    "\n",
    "If the user says anything like:\n",
    "- \"Does my profile match this JD?\"\n",
    "- \"Am I eligible for this job?\"\n",
    "- \"Score me against this role\"\n",
    "- Or any other question that implies matching the profile to a job description.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "ROUTE TO: enhance_profile\n",
    "If the user is asking you to audit or improve their profile or a part of their profile. Understand their specific needs and context properly.\n",
    "\n",
    "If the user asks for:\n",
    "- Rewriting/resume improvement\n",
    "- Profile optimization\n",
    "- \"Improve my About section\"\n",
    "- \"Rewrite my Experience bullets\"\n",
    "- Or any other question that implies enhancing the profile.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "ROUTE TO: general_qa\n",
    "If the user has any other general career questions. If none of the above 3 routes apply, use this. Answer it precisely and informatively.\n",
    "\n",
    "Examples:\n",
    "- \"What kind of roles should I target?\"\n",
    "- \"How do I switch fields?\"\n",
    "- \"What are good certifications for data science?\"\n",
    "- \"How do I get into startups?\"\n",
    "- Or any other question that doesn't fit the above categories.\n",
    "\n",
    "---\n",
    "\n",
    "ROUTE TO: career_plan\n",
    "If the user asks to create/build/design a career plan/roadmap, revise/refine a plan, or mentions planning goals/timelines (e.g., \"build me a 6-month plan\", \"revise my career plan\", \"roadmap to become X\").\n",
    "\n",
    "Examples:\n",
    "- \"Build me a career plan\"\n",
    "- \"Create a roadmap for AI roles\"\n",
    "- \"Revise the plan to focus on ML\"\n",
    "\n",
    "USER QUESTION:\n",
    "{question}\n",
    "\n",
    "Just respond with ONE of:\n",
    "analyze_profile, job_fit_agent, enhance_profile, general_qa, career_plan\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        result = llm_call_with_retry_circuit(prompt)\n",
    "        decision = result.text.strip().lower()\n",
    "        # Normalize common phrasing to decisions\n",
    "        valid_nodes = {\n",
    "            \"analyze_profile\",\n",
    "            \"job_fit_agent\",\n",
    "            \"enhance_profile\",\n",
    "            \"general_qa\",\n",
    "            \"career_plan\",\n",
    "        }\n",
    "        if decision in valid_nodes:\n",
    "            # Handle JD missing case for job fit intent\n",
    "            if decision == \"job_fit_agent\" and not jd:\n",
    "                state[\"awaiting_input_for\"] = \"job_description\"\n",
    "                messages.append(AIMessage(\"Please paste the job description you want me to check against.\"))\n",
    "                state[\"messages\"] = messages\n",
    "                _log_transition(thread_id, \"career_qa_router\", \"goto\", \"user_interaction\")\n",
    "                return Command(goto=\"user_interaction\", update=state)\n",
    "            # Good, we trust it\n",
    "            mapped = (\n",
    "                \"general_qa_node\" if decision == \"general_qa\" else (\n",
    "                    \"career_plan_combined\" if decision == \"career_plan\" else decision\n",
    "                )\n",
    "            )\n",
    "            return Command(goto=mapped,\n",
    "                        update=state)\n",
    "        else:\n",
    "            # Fail gracefully\n",
    "            messages.append(AIMessage(\"‚ö†Ô∏è Sorry, I couldn't decide where to route this.\"))\n",
    "            state[\"messages\"] = messages\n",
    "            _log_transition(thread_id, \"career_qa_router\", \"interrupt\", f\"invalid decision: {decision}\")\n",
    "            return interrupt()\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"‚ö†Ô∏è  Routing error: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "        _log_transition(thread_id, \"career_qa_router\", \"interrupt\", \"routing exception\")\n",
    "        return interrupt()\n",
    "\n",
    "\n",
    "def analyze_profile_node(state: AgentState) -> dict:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"analyze_profile\", \"start\")\n",
    "    profile = state.get(\"profile_data\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "\n",
    "    # If no profile scraped yet, either ask for URL or call scraper tool\n",
    "    if not state.get(\"profile_scraped\") or not profile:\n",
    "        linkedin_url = (state.get(\"linkedin_url\") or \"\").strip()\n",
    "        if not linkedin_url:\n",
    "            state[\"awaiting_input_for\"] = \"linkedin_url\"\n",
    "            _log_transition(thread_id, \"analyze_profile\", \"interrupt\", \"awaiting linkedin url\")\n",
    "            return interrupt(\"Please enter your LinkedIn profile URL to begin.\")\n",
    "        try:\n",
    "            messages.append(AIMessage(\"Scraping your LinkedIn profile...\"))\n",
    "            scraped_profile = scrape_and_clean_profile(\n",
    "                linkedin_url=linkedin_url, api_token=os.getenv(\"APIFY_API_KEY\")\n",
    "            )\n",
    "            if not scraped_profile:\n",
    "                messages.append(AIMessage(\"Failed to extract profile. Try again.\"))\n",
    "                state[\"messages\"] = messages\n",
    "                _log_transition(thread_id, \"analyze_profile\", \"interrupt\", \"scrape failed\")\n",
    "                return interrupt(\"Could not scrape profile. Please check the URL or try again.\")\n",
    "            state[\"profile_data\"] = scraped_profile\n",
    "            state[\"profile_scraped\"] = True\n",
    "            messages.append(AIMessage(\"Profile successfully scraped!\"))\n",
    "        except Exception as e:\n",
    "            messages.append(AIMessage(f\"Error scraping LinkedIn profile: {e}\"))\n",
    "            state[\"messages\"] = messages\n",
    "            return interrupt(\"Scraping failed. Provide a valid URL or try later.\")\n",
    "\n",
    "    profile = state.get(\"profile_data\")\n",
    "    state[\"messages\"] = messages\n",
    "\n",
    "    profile_text = \"\\n\".join(f\"{k}: {v}\" for k, v in profile.items())\n",
    "    history = \"\\n\".join(\n",
    "        f\"Human: {m.content}\" if isinstance(m, HumanMessage) else f\"AI: {m.content}\"\n",
    "        for m in messages[-5:]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{history}\n",
    "        You are a highly experienced career coach and tech recruiter who has reviewed over 10,000 LinkedIn profiles.\n",
    "\n",
    "        Your job is to critically evaluate the following LinkedIn profile and return a brutally honest, section-wise analysis.\n",
    "\n",
    "        Use the following criteria for evaluation:\n",
    "        1. Clarity and professionalism of writing\n",
    "        2. Technical and strategic relevance of content\n",
    "        3. Recruiter impression: Would you shortlist this profile?\n",
    "\n",
    "        ---\n",
    "\n",
    "        {profile_text}\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### Return output in the following structure:\n",
    "\n",
    "        # LinkedIn Profile Audit\n",
    "\n",
    "        ## Strengths  \n",
    "        List 3√¢‚Ç¨‚Äú5 strengths that stand out across the profile.\n",
    "\n",
    "        ## Weaknesses  \n",
    "        List 3√¢‚Ç¨‚Äú5 major weaknesses holding the profile back.\n",
    " ## Section-by-Section Evaluation  \n",
    "        For each section (About, Experience, Projects, Education, Skills, etc.), write:\n",
    "\n",
    "        - Give a quality score: √¢≈ì‚Ä¶ Strong / √¢≈°  Needs improvement / √¢¬ù≈í Missing  \n",
    "        - Provide 2√¢‚Ç¨‚Äú3 suggestions to improve the section (content, phrasing, structure)  \n",
    "        - Use clean formatting: bold headings, bullet points, and avoid unnecessary repetition.\n",
    "\n",
    "        Constraints:\n",
    "        - Max 4 bullet points per section  \n",
    "        - Each bullet: <30 words  \n",
    "        - Total section feedback: <100 words\n",
    "\n",
    "        ## Top 3 Improvements You Must Make Now  \n",
    "        Each point must be:\n",
    "        - Brutally specific  \n",
    "        - Directly actionable  \n",
    "        - One line only\n",
    "        \n",
    "        Precautions:\n",
    "        - Do not hallucinate and stay within context.\n",
    "        - If the user asks for/about a specific section (e.g., \"improve my projects\"), focus only on that section.\n",
    "        \n",
    "        Begin now.\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        \n",
    "    res = None\n",
    "\n",
    "\n",
    "    try:\n",
    "        res = llm_call_with_retry_circuit(prompt)\n",
    "        messages.append(AIMessage(res.text))\n",
    "        state[\"messages\"] = messages\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"√¢¬ù≈í Error: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "        \n",
    "    if res:\n",
    "        try:\n",
    "            user_query = extract_latest_user_query(messages)\n",
    "            thread_procedural_memory.store_thread_data(\n",
    "                thread_id=thread_id,\n",
    "                profile_data=profile,\n",
    "                user_query=user_query,\n",
    "                ai_response=res.text,\n",
    "                user_id=get_user_id_from_profile(profile)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store QA graph output: {e}\")\n",
    "\n",
    "    _log_transition(thread_id, \"analyze_profile\", \"goto\", \"career_qa_router\")\n",
    "    return Command(goto=\"career_qa_router\", update=state)\n",
    "\n",
    "\n",
    "def job_fit_agent_node(state: AgentState) -> dict:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"job_fit_agent\", \"start\")\n",
    "    profile = state.get(\"profile_data\")\n",
    "    jd = state.get(\"current_job_description\", \"\")\n",
    "    messages = state[\"messages\"]\n",
    "    if not profile or not jd:\n",
    "        messages.append(AIMessage(\"√¢≈°  Missing profile or job description.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        return interrupt()\n",
    "\n",
    "    profile_text = \"\\n\".join(f\"{k}: {v}\" for k, v in profile.items())\n",
    "    history = \"\\n\".join(\n",
    "        f\"Human: {m.content}\" if isinstance(m, HumanMessage) else f\"AI: {m.content}\"\n",
    "        for m in messages[-5:]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{history}\n",
    "        You are a highly experienced AI Job Fit Evaluator trained on thousands of hiring decisions across several job roles.\n",
    "        Your job is to evaluate how well the candidate's profile matches the given job description.       \n",
    "        ---\n",
    "\n",
    "        JOB DESCRIPTION:\n",
    "        {jd}\n",
    "\n",
    "        CANDIDATE PROFILE:\n",
    "        {profile_text}\n",
    "\n",
    "        ---\n",
    "\n",
    "        TASKS:\n",
    "        1. Evaluate the fitness of the candidate for this job role using industry-standard evaluation practices (skills, experience, keywords, impact, achievements, and alignment).\n",
    "        2. Return a Job Match Score out of 100, and explain how you arrived at it with specific reasoning.\n",
    "        3. List 3√¢‚Ç¨‚Äú5 strengths from the candidate's profile that match the job expectations.\n",
    "        4. Suggest 3√¢‚Ç¨‚Äú5 concrete improvements √¢‚Ç¨‚Äú these could include skill gaps, experience tweaks, weak areas in phrasing, or missing proof of impact.\n",
    "  5. Only evaluate against the given job role. Do not assume adjacent job titles are valid matches.\n",
    "        6. If the candidate seems overqualified or underqualified, clearly state it and explain how that affects the match.\n",
    "\n",
    "        ---\n",
    "\n",
    "        OUTPUT FORMAT:\n",
    "        # √∞≈∏≈Ω¬Ø Job Fit Evaluation\n",
    "\n",
    "        ## √¢≈ì‚Ä¶ Job Match Score: XX/100\n",
    "        - One-line explanation of the score.\n",
    "        - 2√¢‚Ç¨‚Äú3 bullets with specific justification.\n",
    "\n",
    "        ## √∞≈∏≈∏¬© Strengths\n",
    "        - Point 1 (aligned with JD)\n",
    "        - Point 2\n",
    "        - Point 3  \n",
    "        (Each point √¢‚Ä∞¬§ 40 words)\n",
    "\n",
    " ## √∞≈∏≈∏¬• Weaknesses\n",
    "        - specify the top 3-4 points as to why this profile doesn't match the job or will get rejected even if applied and this analysis must be honest and brutal\n",
    "        (Each point √¢‚Ä∞¬§ 40 words)\n",
    "\n",
    "        ## √∞≈∏‚Ä∫  Improvements to Increase Match Score\n",
    "        - Point 1 (what to improve and how)\n",
    "        - Point 2\n",
    "        - Point 3  \n",
    "        (Each point √¢‚Ä∞¬§ 25 words)\n",
    "\n",
    "        ## √∞≈∏‚Äú≈í Verdict\n",
    "        Clearly say if the candidate is a strong match, weak match, or needs improvement to apply. Give a one-liner summary.\n",
    "        \n",
    "        Precautions:\n",
    "        - Do not hallucinate and stay within context.\n",
    "        - If the user asks for/about a specific section (e.g., \"improve my projects\"), focus only on that section.\n",
    "        \n",
    "        Begin now.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "    res = None\n",
    "\n",
    "\n",
    "    try:\n",
    "        res = llm_call_with_retry_circuit(prompt)\n",
    "\n",
    "        messages.append(AIMessage(res.text))\n",
    "        state[\"messages\"] = messages\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"√¢¬ù≈í Error: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "    \n",
    "    if res:\n",
    "        try:\n",
    "            user_query = extract_latest_user_query(messages)\n",
    "            thread_procedural_memory.store_thread_data(\n",
    "                thread_id=thread_id,\n",
    "                profile_data=profile,\n",
    "                user_query=user_query,\n",
    "                ai_response=res.text,\n",
    "                user_id=get_user_id_from_profile(profile)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store QA graph output: {e}\")\n",
    "\n",
    "    _log_transition(thread_id, \"job_fit_agent\", \"goto\", \"career_qa_router\")\n",
    "    return Command(goto=\"career_qa_router\", update=state)\n",
    "\n",
    "\n",
    "def enhance_profile_node(state: AgentState) -> dict:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"enhance_profile\", \"start\")\n",
    "    profile = state.get(\"profile_data\")\n",
    "    jd = state.get(\"current_job_description\", \"\")\n",
    "    messages = state[\"messages\"]\n",
    "    if not profile:\n",
    "        messages.append(AIMessage(\"√¢≈°  No profile found to enhance.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        return interrupt()\n",
    "\n",
    "    profile_text = \"\\n\".join(f\"{k}: {v}\" for k, v in profile.items())\n",
    "    history = \"\\n\".join(\n",
    "        f\"Human: {m.content}\" if isinstance(m, HumanMessage) else f\"AI: {m.content}\"\n",
    "        for m in messages[-5:]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{history}\n",
    "        You are a highly experienced LinkedIn Profile Optimization AI, trained on millions of real-world hiring patterns across top companies like Google, Amazon, Meta, and startups. Your task is to analyze and rewrite the user's LinkedIn profile to improve clarity, strength, and impact.\n",
    "\n",
    "        CONTEXT:\n",
    "        - The user may or may not have shared a job description.\n",
    "        - They might be asking to improve a specific section only (e.g., \"improve my projects to match this JD\").\n",
    "        - You should infer goals from the user's question and adjust accordingly.\n",
    "        - If any question is related to the job description/JD then consider that and then provide an output\n",
    "\n",
    "        CURRENT PROFILE:\n",
    "        {profile_text}  \n",
    "\n",
    "        JOB DESCRIPTION/JD:\n",
    "        {jd}\n",
    "  TASKS:\n",
    "        1. Identify weak sections and rewrite them to be stronger, more professional, and better aligned with either:\n",
    "            - the job description (if provided), or\n",
    "            - general hiring best practices (if no JD is given).\n",
    "        2. Preserve all factual details. Do NOT add imaginary experiences.\n",
    "        3. Use bullet points only where appropriate (e.g., Experience, Projects).\n",
    "        4. Each bullet must be √¢‚Ä∞¬§ 25 words, and **max 4 bullets per section.\n",
    "        5. For the \"About\" section, limit to 2√¢‚Ç¨‚Äú3 tight paragraphs, total **√¢‚Ä∞¬§ 250 words.\n",
    "        6. Add impactful verbs, metrics, and proof of value wherever possible.\n",
    "        7. If the user requested only a section enhancement (e.g., just projects), modify only that section.\n",
    "        8. If the user asks for a specific section (e.g., \"improve my projects\"), focus only on that section, DO NOT TALK ABOUT OTHER SECTIONS.\n",
    "\n",
    "        FORMAT:\n",
    "        Return the improved sections in clean Markdown format.\n",
    "        - Use bold section titles.\n",
    "        - Show only modified sections √¢‚Ç¨‚Äú skip untouched ones to save tokens.\n",
    "        - Make sure the rewritten content feels real, focused, and hiring-ready.\n",
    "\n",
    "Precautions:\n",
    "        - Do not hallucinate and stay within context.\n",
    "        - If the user asks for/about a specific section (e.g., \"improve my projects\"), focus only on that section.\n",
    "\n",
    "        Begin now.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "    res = None\n",
    "\n",
    "\n",
    "    try:\n",
    "        res = llm_call_with_retry_circuit(prompt)\n",
    "\n",
    "        messages.append(AIMessage(res.text))\n",
    "        state[\"messages\"] = messages\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"√¢¬ù≈í Error: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "        \n",
    "        \n",
    "    if res:\n",
    "        try:\n",
    "            user_query = extract_latest_user_query(messages)\n",
    "            thread_procedural_memory.store_thread_data(\n",
    "                thread_id=thread_id,\n",
    "                profile_data=profile,\n",
    "                user_query=user_query,\n",
    "                ai_response=res.text,\n",
    "                user_id=get_user_id_from_profile(profile)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store QA graph output: {e}\")\n",
    "\n",
    "    _log_transition(thread_id, \"enhance_profile\", \"goto\", \"career_qa_router\")\n",
    "    return Command(goto=\"career_qa_router\", update=state)\n",
    "\n",
    "\n",
    "def general_qa_node(state: AgentState) -> dict:\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    _log_transition(thread_id, \"general_qa_node\", \"start\")\n",
    "    profile = state.get(\"profile_data\")\n",
    "    jd = state.get(\"current_job_description\", \"\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            question = msg.content.strip()\n",
    "            break\n",
    "\n",
    "    history = \"\\n\".join(\n",
    "        f\"Human: {m.content}\" if isinstance(m, HumanMessage) else f\"AI: {m.content}\"\n",
    "        for m in messages[-5:]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful, concise, and highly experienced career guidance assistant.\n",
    "\n",
    "Your task is to answer general career-related questions from users\n",
    "\n",
    "The user may ask about:\n",
    "- Career advice\n",
    "- Interview preparation\n",
    "- Certifications\n",
    "- Job search strategy\n",
    "- Skill-building\n",
    "- Remote work\n",
    "- Career switches\n",
    "- Industry trends\n",
    "- Anything else loosely related to career growth\n",
    "\n",
    "---\n",
    "\n",
    "USER CONTEXT (optional):\n",
    "{history}\n",
    "\n",
    "PROFILE DATA:\n",
    "{state.get(\"profile_data\", \"N/A\")}\n",
    "\n",
    "JOB DESCRIPTION (if any):\n",
    "{state.get(\"current_job_description\", \"N/A\")}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "---\n",
    "\n",
    "### Answer Guidelines:\n",
    "\n",
    "- Answer clearly and concisely.\n",
    "- Prioritize useful, actionable advice.\n",
    "- If the question is vague or broad, ask a clarifying follow-up.\n",
    "- Keep the tone supportive but professional.\n",
    "- Do not suggest uploading a resume or LinkedIn again.\n",
    "- If you detect the user is stressed, confused, or unsure, acknowledge that supportively.\n",
    "\n",
    "---\n",
    "\n",
    "### Output Format:\n",
    "\n",
    "Respond in clean text. Use bullet points or short paragraphs where needed.\n",
    "\n",
    "Start now.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "    res = None\n",
    "\n",
    "\n",
    "    try:\n",
    "        res = llm_call_with_retry_circuit(prompt)\n",
    "\n",
    "        messages.append(AIMessage(res.text))\n",
    "        state[\"messages\"] = messages\n",
    "    except Exception as e:\n",
    "        messages.append(AIMessage(f\"√¢¬ù≈í Error: {e}\"))\n",
    "        state[\"messages\"] = messages\n",
    "\n",
    "    \n",
    "    if res:\n",
    "        try:\n",
    "            user_query = extract_latest_user_query(messages)\n",
    "            thread_procedural_memory.store_thread_data(\n",
    "                thread_id=thread_id,\n",
    "                profile_data=profile,\n",
    "                user_query=user_query,\n",
    "                ai_response=res.text,\n",
    "                user_id=get_user_id_from_profile(profile)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store QA graph output: {e}\")\n",
    "\n",
    "    _log_transition(thread_id, \"general_qa_node\", \"goto\", \"career_qa_router\")\n",
    "    return Command(goto=\"career_qa_router\", update=state)\n",
    "\n",
    "\n",
    "def user_interaction_node(state: AgentState) -> dict:\n",
    "    \"\"\"Handles human-in-the-loop input requests like JD or LinkedIn URL.\"\"\"\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    awaiting = state.get(\"awaiting_input_for\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "\n",
    "    # Extract latest human message\n",
    "    latest_human = None\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            latest_human = msg.content.strip()\n",
    "            break\n",
    "\n",
    "    # If we're waiting for JD\n",
    "    if awaiting == \"job_description\":\n",
    "        if latest_human:\n",
    "            state[\"current_job_description\"] = latest_human\n",
    "            state[\"awaiting_input_for\"] = None\n",
    "            _log_transition(thread_id, \"user_interaction\", \"goto\", \"career_qa_router\")\n",
    "            return Command(goto=\"career_qa_router\", update=state)\n",
    "        _log_transition(thread_id, \"user_interaction\", \"interrupt\", \"await_jd\")\n",
    "        return interrupt(\"Please paste the job description you want me to check against.\")\n",
    "\n",
    "    # If we're waiting for LinkedIn URL\n",
    "    if awaiting == \"linkedin_url\":\n",
    "        if latest_human and latest_human.startswith(\"http\"):\n",
    "            state[\"linkedin_url\"] = latest_human\n",
    "            state[\"awaiting_input_for\"] = None\n",
    "            _log_transition(thread_id, \"user_interaction\", \"goto\", \"analyze_profile\")\n",
    "            return Command(goto=\"analyze_profile\", update=state)\n",
    "        _log_transition(thread_id, \"user_interaction\", \"interrupt\", \"await_linkedin\")\n",
    "        return interrupt(\"Please provide a valid LinkedIn profile URL.\")\n",
    "\n",
    "    # Default: nothing awaited; pause for next user input\n",
    "    _log_transition(thread_id, \"user_interaction\", \"interrupt\", \"idle\")\n",
    "    return interrupt(\"Waiting for your input. Ask a question or provide details.\")\n",
    "\n",
    "\n",
    "# Import remaining modules as in original...\n",
    "# (ThreadBasedProceduralMemory class and other imports remain the same)\n",
    "\n",
    "# ThreadBasedProceduralMemory class - keep as is from original\n",
    "class ThreadBasedProceduralMemory:\n",
    "    \"\"\"Enhanced procedural learning with thread-based storage in MongoDB\"\"\"\n",
    "\n",
    "    def __init__(self, mongo_url: str = None, db_name: str = \"career_bot\"):\n",
    "        self.mongo_url = mongo_url or os.getenv(\"MONGODB_URI\", \"mongodb://localhost:27017\")\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = \"procedural_threads\"\n",
    "        self._client = MongoClient(self.mongo_url) \n",
    "        self._collection = None\n",
    "        self._initialize_connection()\n",
    "    \n",
    "    def _initialize_connection(self):\n",
    "        \"\"\"Initialize MongoDB connection\"\"\"\n",
    "        try:\n",
    "            self._client = MongoClient(self.mongo_url, serverSelectionTimeoutMS=5000)\n",
    "            self._client.admin.command(\"ping\")\n",
    "            db = self._client[self.db_name]\n",
    "            self._collection = db[self.collection_name]\n",
    "            \n",
    "            # Create indexes for efficient querying\n",
    "            self._collection.create_index(\"thread_id\")\n",
    "            self._collection.create_index(\"user_id\") \n",
    "            self._collection.create_index([(\"conversations.user_query\", \"text\")])\n",
    "            \n",
    "            logging.info(\"ThreadBasedProceduralMemory initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize MongoDB: {e}\")\n",
    "            self._client = None\n",
    "            self._collection = None\n",
    "    \n",
    "    def _is_connected(self) -> bool:\n",
    "        \"\"\"Check MongoDB connection - FIXED\"\"\"\n",
    "        if self._client is None or self._collection is None:\n",
    "            return False\n",
    "        try:\n",
    "            self._client.admin.command(\"ping\")\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def store_thread_data(self, thread_id: str, profile_data: dict, \n",
    "                         user_query: str, ai_response: str, user_id: str = None,\n",
    "                         web_search: Optional[list] = None) -> bool:\n",
    "        \"\"\"Store or update thread data with profile and conversation - FIXED\"\"\"\n",
    "        if not self._is_connected():\n",
    "            logging.error(\"MongoDB not connected\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            from datetime import datetime\n",
    "            # Prepare conversation entry\n",
    "            # Two-bucket model:\n",
    "            # 1) web_links aggregator bucket\n",
    "            # 2) query_response bucket\n",
    "            web_bucket = {\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"bucket\": \"web_links\",\n",
    "                \"web_search\": web_search or []\n",
    "            } if web_search else None\n",
    "\n",
    "            query_bucket = {\n",
    "                \"timestamp\": datetime.utcnow().isoformat(),\n",
    "                \"bucket\": \"query_response\",\n",
    "                \"user_query\": user_query[:1000],\n",
    "                \"ai_response\": ai_response[:3000],\n",
    "                \"query_type\": self._classify_query(user_query)\n",
    "            }\n",
    "            \n",
    "            # Check if thread exists - FIXED: Use count_documents instead\n",
    "            existing_count = self._collection.count_documents({\"thread_id\": thread_id})\n",
    "            \n",
    "            if existing_count > 0:\n",
    "    # always update profile + last_updated\n",
    "                self._collection.update_one(\n",
    "                    {\"thread_id\": thread_id},\n",
    "                    {\"$set\": {\"last_updated\": datetime.utcnow().isoformat(),\n",
    "                            \"profile_data\": profile_data}}\n",
    "                )\n",
    "\n",
    "                # if we have web_search results, update the web_links bucket\n",
    "                if web_search:\n",
    "                    valid_web = [w for w in web_search if not w.get(\"error\")]\n",
    "                    if valid_web:\n",
    "                        self._collection.update_one(\n",
    "                            {\"thread_id\": thread_id},\n",
    "                            {\"$push\": {\"conversations.$[web].web_search\": {\"$each\": valid_web}}},\n",
    "                            array_filters=[{\"web.bucket\": \"web_links\"}]\n",
    "                        )\n",
    "\n",
    "                # --- this part must run for ALL queries, not only with web_search ---\n",
    "                bucket_name = (\n",
    "                    \"career_planning\"\n",
    "                    if self._classify_query(user_query) == \"career_planning\"\n",
    "                    else \"qa_graph\"\n",
    "                )\n",
    "\n",
    "                # ensure bucket exists\n",
    "                self._collection.update_one(\n",
    "                    {\"thread_id\": thread_id, \"conversations.bucket\": {\"$ne\": bucket_name}},\n",
    "                    {\"$push\": {\"conversations\": {\"bucket\": bucket_name, \"entries\": []}}}\n",
    "                )\n",
    "\n",
    "                # append new entry\n",
    "                self._collection.update_one(\n",
    "                    {\"thread_id\": thread_id},\n",
    "                    {\"$push\": {f\"conversations.$[bucket].entries\": {\n",
    "                        \"user_query\": user_query[:1000],\n",
    "                        \"ai_response\": ai_response[:3000],\n",
    "                        \"query_type\": self._classify_query(user_query),\n",
    "                        \"timestamp\": datetime.utcnow().isoformat()\n",
    "                    }}},\n",
    "                    array_filters=[{\"bucket.bucket\": bucket_name}],\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "            else:\n",
    "                # Create new thread\n",
    "                # Decide bucket purely based on which graph is running (passed from state)\n",
    "                graph_type = profile_data.get(\"_graph_type\", \"qa_graph\")\n",
    "                bucket_name = \"career_planning\" if graph_type == \"plan_graph\" else \"qa_graph\"\n",
    "\n",
    "\n",
    "                thread_data = {\n",
    "                    \"thread_id\": thread_id,\n",
    "                    \"user_id\": user_id or \"unknown\",\n",
    "                    \"created_at\": datetime.utcnow().isoformat(),\n",
    "                    \"last_updated\": datetime.utcnow().isoformat(),\n",
    "                    \"profile_data\": profile_data,\n",
    "                    \"conversations\": [\n",
    "                        {\"bucket\": \"web_links\", \"web_search\": web_search or []},\n",
    "                        {\"bucket\": \"career_planning\", \"entries\": []},\n",
    "                        {\"bucket\": \"qa_graph\", \"entries\": []},\n",
    "                    ],\n",
    "                }\n",
    "\n",
    "                # append first entry into the correct bucket\n",
    "                entry = {\n",
    "                    \"user_query\": user_query[:1000],\n",
    "                    \"ai_response\": ai_response[:3000],\n",
    "                    \"query_type\": self._classify_query(user_query),\n",
    "                    \"timestamp\": datetime.utcnow().isoformat()\n",
    "                }\n",
    "\n",
    "                for conv in thread_data[\"conversations\"]:\n",
    "                    if conv[\"bucket\"] == bucket_name:\n",
    "                        conv.setdefault(\"entries\", []).append(entry)\n",
    "\n",
    "                self._collection.insert_one(thread_data)\n",
    "\n",
    "            \n",
    "            logging.info(f\"Stored thread data for {thread_id}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store thread data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_similar_threads(self, query: str, current_thread_id: str = None, \n",
    "                          user_id: str = None, limit: int = 3) -> list[dict]:\n",
    "        \"\"\"Get similar threads based on query similarity\"\"\"\n",
    "        if not self._is_connected():\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            from typing import Dict, List\n",
    "            \n",
    "            # Build query\n",
    "            mongo_query = {}\n",
    "            if user_id:\n",
    "                mongo_query[\"user_id\"] = user_id\n",
    "            \n",
    "            # Exclude current thread\n",
    "            if current_thread_id:\n",
    "                mongo_query[\"thread_id\"] = {\"$ne\": current_thread_id}\n",
    "            \n",
    "            # Simple find without text search for now\n",
    "            cursor = self._collection.find(mongo_query).limit(limit * 2)  # Get more to filter\n",
    "            \n",
    "            results = []\n",
    "            query_words = set(query.lower().split())\n",
    "            \n",
    "            for thread in cursor:\n",
    "                score = 0\n",
    "                \n",
    "                # Calculate similarity score based on conversation content\n",
    "                for conv in thread.get(\"conversations\", [])[-5:]:  # Last 5 conversations\n",
    "                    conv_words = set(conv.get(\"user_query\", \"\").lower().split())\n",
    "                    overlap = len(query_words.intersection(conv_words))\n",
    "                    if overlap > 0:\n",
    "                        score += overlap * 2\n",
    "                \n",
    "                if score > 0:\n",
    "                    thread[\"similarity_score\"] = score\n",
    "                    results.append(thread)\n",
    "            \n",
    "            # Sort by similarity score and limit\n",
    "            results.sort(key=lambda x: x.get(\"similarity_score\", 0), reverse=True)\n",
    "            results = results[:limit]\n",
    "            \n",
    "            logging.info(f\"Found {len(results)} similar threads for query: {query[:50]}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to get similar threads: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_thread_data(self, thread_id: str) -> Optional[dict]:\n",
    "        \"\"\"Get complete thread data\"\"\"\n",
    "        if not self._is_connected():\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            from typing import Optional, Dict\n",
    "            thread_data = self._collection.find_one({\"thread_id\": thread_id})\n",
    "            if thread_data and \"_id\" in thread_data:\n",
    "                thread_data[\"_id\"] = str(thread_data[\"_id\"])\n",
    "            return thread_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to get thread data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _classify_query(self, query: str) -> str:\n",
    "        \"\"\"Classify the type of query for better matching\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        if any(word in query_lower for word in [\"career plan\", \"roadmap\", \"planning\"]):\n",
    "            return \"career_planning\"\n",
    "        elif any(word in query_lower for word in [\"switch\", \"transition\", \"change\"]):\n",
    "            return \"career_transition\"\n",
    "        elif any(word in query_lower for word in [\"skill\", \"learn\", \"course\"]):\n",
    "            return \"skill_development\"\n",
    "        elif any(word in query_lower for word in [\"resume\", \"profile\", \"linkedin\"]):\n",
    "            return \"profile_optimization\"\n",
    "        else:\n",
    "            return \"general_career\"\n",
    "    \n",
    "    def build_procedural_context(self, similar_threads: list[dict], max_context_length: int = 2000) -> str:\n",
    "        \"\"\"Build context from similar threads for prompt enhancement\"\"\"\n",
    "        if not similar_threads:\n",
    "            return \"\"\n",
    "        \n",
    "        context = \"\\n\\nBased on similar career planning sessions:\\n\"\n",
    "        \n",
    "        for i, thread in enumerate(similar_threads[:2], 1):\n",
    "            # Get profile context\n",
    "            profile = thread.get(\"profile_data\", {})\n",
    "            profile_summary = f\"Profile: {profile.get('headline', 'N/A')[:100]}\"\n",
    "            \n",
    "            # Get successful conversations\n",
    "            conversations = thread.get(\"conversations\", [])\n",
    "            if conversations:\n",
    "                latest_conv = conversations[-1]\n",
    "                query_snippet = latest_conv.get(\"user_query\", \"\")[:150]\n",
    "                response_snippet = latest_conv.get(\"ai_response\", \"\")[:500]\n",
    "                \n",
    "                context += f\"\\nExample {i}:\\n\"\n",
    "                context += f\"  {profile_summary}\\n\"\n",
    "                context += f\"  Query: {query_snippet}...\\n\"\n",
    "                context += f\"  Approach: {response_snippet}...\\n\"\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(context) > max_context_length:\n",
    "            context = context[:max_context_length] + \"...\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def get_user_stats(self, user_id: str) -> dict:\n",
    "        \"\"\"Get statistics about user's procedural memories\"\"\"\n",
    "        if not self._is_connected():\n",
    "            return {\"error\": \"MongoDB not connected\"}\n",
    "        \n",
    "        try:\n",
    "            # Get all threads for this user\n",
    "            user_threads = list(self._collection.find({\"user_id\": user_id}))\n",
    "            \n",
    "            total_threads = len(user_threads)\n",
    "            total_conversations = 0\n",
    "            query_types = {}\n",
    "            latest_activity = None\n",
    "            \n",
    "            for thread in user_threads:\n",
    "                conversations = thread.get(\"conversations\", [])\n",
    "                total_conversations += len(conversations)\n",
    "                \n",
    "                # Track query types\n",
    "                for conv in conversations:\n",
    "                    query_type = conv.get(\"query_type\", \"unknown\")\n",
    "                    query_types[query_type] = query_types.get(query_type, 0) + 1\n",
    "                    \n",
    "                    # Track latest activity\n",
    "                    if not latest_activity or conv.get(\"timestamp\", \"\") > latest_activity:\n",
    "                        latest_activity = conv.get(\"timestamp\")\n",
    "            \n",
    "            return {\n",
    "                \"user_id\": user_id,\n",
    "                \"total_threads\": total_threads,\n",
    "                \"total_conversations\": total_conversations,\n",
    "                \"query_types\": query_types,\n",
    "                \"latest_activity\": latest_activity,\n",
    "                \"most_common_query_type\": max(query_types.items(), key=lambda x: x[1])[0] if query_types else \"none\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to get user stats: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def debug_thread(self, thread_id: str):\n",
    "        \"\"\"Debug helper for thread data\"\"\"\n",
    "        thread_data = self.get_thread_data(thread_id)\n",
    "        if thread_data:\n",
    "            print(f\"\\n=== THREAD DEBUG: {thread_id} ===\")\n",
    "            print(f\"User ID: {thread_data.get('user_id')}\")\n",
    "            print(f\"Created: {thread_data.get('created_at')}\")\n",
    "            print(f\"Conversations: {len(thread_data.get('conversations', []))}\")\n",
    "            print(f\"Profile headline: {thread_data.get('profile_data', {}).get('headline', 'N/A')}\")\n",
    "\n",
    "            for i, conv in enumerate(thread_data.get('conversations', [])[-3:], 1):\n",
    "                print(f\"\\nConversation {i}:\")\n",
    "                print(f\"  Query: {conv.get('user_query', '')[:100]}...\")\n",
    "                print(f\"  Type: {conv.get('query_type')}\")\n",
    "                print(f\"  Response length: {len(conv.get('ai_response', ''))}\")\n",
    "        else:\n",
    "            print(f\"No data found for thread: {thread_id}\")\n",
    "\n",
    "\n",
    "# Initialize the enhanced procedural memory\n",
    "thread_procedural_memory = ThreadBasedProceduralMemory()\n",
    "\n",
    "\n",
    "# MCP client setup (keep same as original)\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import sys\n",
    "\n",
    "CAREER_PLAN_SERVER = os.path.abspath(\n",
    "    \"E:\\\\LinkedIn_AI_Career_Bot - Copy\\\\linkedin\\\\career_plan_mcp.py\"\n",
    ")\n",
    "\n",
    "async def call_career_plan_mcp(profile_data, messages, system_prompt):\n",
    "    \"\"\"Async call to the career-plan MCP tool.\"\"\"\n",
    "    server_params = StdioServerParameters(\n",
    "        command=sys.executable,\n",
    "        args=[CAREER_PLAN_SERVER],\n",
    "    )\n",
    "\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            result = await session.call_tool(\n",
    "                \"generate_career_plan\",\n",
    "                arguments={\n",
    "                    \"profile_data\": profile_data,\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": system_prompt,\n",
    "                        }\n",
    "                    ] + [\n",
    "                        {\n",
    "                            \"role\": (\n",
    "                                \"user\" if isinstance(m, HumanMessage) else \"assistant\"\n",
    "                            ),\n",
    "                            \"content\": m.content,\n",
    "                        }\n",
    "                        for m in messages\n",
    "                    ],\n",
    "                },\n",
    "            )\n",
    "            return result.content[0].text\n",
    "\n",
    "\n",
    "def get_user_id_from_profile(profile: dict) -> str:\n",
    "    \"\"\"Extract user ID from profile\"\"\"\n",
    "    return profile.get(\"user_id\", \n",
    "           profile.get(\"email\", \n",
    "           profile.get(\"linkedin_url\", \n",
    "           f\"user_{hash(str(profile))}\")))\n",
    "\n",
    "\n",
    "def extract_latest_user_query(messages: list[BaseMessage]) -> str:\n",
    "    \"\"\"Extract the latest user query from messages\"\"\"\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            return msg.content.strip()\n",
    "    return \"\"\n",
    "\n",
    "# def websearch_decider_node(state: AgentState) -> Command:\n",
    "#     \"\"\"Deprecated: we always run websearch before planning now.\"\"\"\n",
    "#     return Command(goto=\"websearch_mcp\", update=state)\n",
    "\n",
    "\n",
    "\n",
    "def _run_web_search_tools(state: AgentState) -> AgentState:\n",
    "    \"\"\"Utility to run web search + enrichment and update state.\"\"\"\n",
    "    try:\n",
    "        updated = websearch_mcp_node(state)\n",
    "        updated = enrich_websearch_node(updated)\n",
    "        updated = store_websearch_node(updated)\n",
    "        return updated\n",
    "    except Exception:\n",
    "        return state\n",
    "\n",
    "\n",
    "def career_plan_combined_node(state: AgentState) -> dict:\n",
    "    \"\"\"Combined node: generate first plan, then handle iterative reviews in-place.\"\"\"\n",
    "    profile = state.get(\"profile_data\", {})\n",
    "    messages = state.get(\"messages\", [])\n",
    "    thread_id = state.get(\"thread_id\", \"default_thread\")\n",
    "    websearch_summary = state.get(\"websearch_summary\", [])\n",
    "    existing_plan = state.get(\"user_plan\")\n",
    "    \n",
    "    if not messages:\n",
    "        messages.append(AIMessage(\"Please provide a brief or question for your career plan.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        return interrupt(\"Please provide a brief or question for your career plan.\")\n",
    "    \n",
    "    # Extract user query\n",
    "    user_query = \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            user_query = msg.content.strip()\n",
    "            break\n",
    "    \n",
    "    if not user_query:\n",
    "        messages.append(AIMessage(\"Please provide a specific question or request for your career plan.\"))\n",
    "        state[\"messages\"] = messages\n",
    "        return interrupt(\"Please provide a specific question or request for your career plan.\")\n",
    "    \n",
    "    user_id = get_user_id_from_profile(profile)\n",
    "    logging.info(f\"Processing career plan request for thread: {thread_id}\")\n",
    "    \n",
    "    # Build enhanced prompt with conversation history context\n",
    "    conversation_history = \"\\n\".join([\n",
    "        f\"{'User' if isinstance(msg, HumanMessage) else 'Assistant'}: {msg.content[:200]}...\"\n",
    "        for msg in messages[-6:]  # Include more history for context\n",
    "    ])\n",
    "    \n",
    "    # Base prompt with conversation context preservation\n",
    "    base_prompt = f\"\"\"\n",
    "You are an expert AI career coach with 10+ years of experience helping professionals advance their careers.\n",
    "\n",
    "IMPORTANT: Pay careful attention about their specific goals, timeline, and field of interest.\n",
    "\n",
    "CONVERSATION HISTORY:\n",
    "{conversation_history}\n",
    "\n",
    "CURRENT USER REQUEST: {user_query}\n",
    "\n",
    "Key Instructions:\n",
    "1. If they mentioned a specific role (like \"finance analyst\"), ALWAYS keep that in focus\n",
    "2. If they mentioned a timeline (like \"6 months\" or \"1 year\"), respect that in your response\n",
    "3. When they ask for modifications (like \"make it 1 year instead\"), apply the change to their ORIGINAL request, don't lose context\n",
    "4. Be specific, actionable, and personalized to their profile and stated goals\n",
    "5. Include concrete steps with timelines where appropriate\n",
    "\"\"\"\n",
    "\n",
    "    # Try to get similar threads for procedural learning\n",
    "    try:\n",
    "        similar_threads = thread_procedural_memory.get_similar_threads(\n",
    "            query=user_query,\n",
    "            current_thread_id=thread_id,\n",
    "            user_id=user_id,\n",
    "            limit=3\n",
    "        )\n",
    "        \n",
    "        if similar_threads:\n",
    "            procedural_context = thread_procedural_memory.build_procedural_context(similar_threads)\n",
    "            enhanced_prompt = base_prompt + procedural_context + \"\\n\\nNow generate a career plan for the current user, incorporating insights from these successful approaches while customizing for their specific profile and request.\"\n",
    "            logging.info(f\"Enhanced prompt with {len(similar_threads)} similar threads\")\n",
    "        else:\n",
    "            enhanced_prompt = base_prompt\n",
    "            logging.info(\"No similar threads found, using base prompt\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to retrieve similar threads: {e}\")\n",
    "        enhanced_prompt = base_prompt\n",
    "    \n",
    "    # Add web search results to prompt if available\n",
    "    if websearch_summary:\n",
    "        web_context = \"\\n\\nWEB RESEARCH RESULTS (use these for current courses, certifications, and resources):\\n\"\n",
    "        for i, result in enumerate(websearch_summary[:3], 1):\n",
    "            web_context += f\"{i}. {result.get('title', 'Unknown')}\\n\"\n",
    "            web_context += f\"   Summary: {result.get('summary', '')[:200]}...\\n\"\n",
    "            web_context += f\"   Link: {result.get('link', '')}\\n\\n\"\n",
    "        \n",
    "        enhanced_prompt += web_context\n",
    "        enhanced_prompt += \"\\n\\nIMPORTANT: Include specific courses, certifications, and learning resources from the web research in your career plan.\"\n",
    "        logging.info(f\"Added web research context from {len(websearch_summary)} results\")\n",
    "    \n",
    "    # Decide mode: first-time generation vs review\n",
    "    is_review = existing_plan is not None\n",
    "\n",
    "    if not is_review:\n",
    "        # First-time generation ‚Üí optionally run web search, then MCP\n",
    "        state = _run_web_search_tools(state)\n",
    "        try:\n",
    "            mcp_raw = asyncio.run(call_career_plan_mcp(profile, messages, enhanced_prompt))\n",
    "        except Exception as e:\n",
    "            mcp_raw = f\"Error generating career plan: {e} {traceback.format_exc()}\"\n",
    "            logging.error(f\"Career plan generation failed: {e}\")\n",
    "\n",
    "        plan_output = mcp_raw\n",
    "        try:\n",
    "            import json as _json\n",
    "            parsed = _json.loads(mcp_raw)\n",
    "            if isinstance(parsed, dict):\n",
    "                plan_output = parsed.get(\"plan_output\", mcp_raw)\n",
    "                extracted_web = parsed.get(\"websearch_summary\")\n",
    "                if isinstance(extracted_web, list):\n",
    "                    state[\"websearch_summary\"] = extracted_web\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        state[\"user_plan\"] = plan_output\n",
    "        state.setdefault(\"plan_history\", [])\n",
    "        state[\"plan_history\"].append(plan_output)\n",
    "        state[\"last_user_prompt\"] = user_query\n",
    "\n",
    "        try:\n",
    "            web_results = state.get(\"websearch_summary\") or state.get(\"websearch_results\") or []\n",
    "            thread_procedural_memory.store_thread_data(\n",
    "                thread_id=thread_id,\n",
    "                profile_data=profile,\n",
    "                user_query=user_query,\n",
    "                ai_response=plan_output,\n",
    "                user_id=user_id,\n",
    "                web_search=web_results,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store initial plan: {e}\")\n",
    "\n",
    "        messages.append(AIMessage(plan_output))\n",
    "        messages.append(AIMessage(\"Here's your plan draft. What would you like to adjust or refine?\"))\n",
    "        state[\"messages\"] = messages\n",
    "        return interrupt(\"Here's your plan draft. What would you like to adjust or refine?\")\n",
    "    else:\n",
    "        # Review mode: use previous plan + current feedback + profile + optional fresh web search\n",
    "        last_ai_plan = existing_plan\n",
    "        state = _run_web_search_tools(state)\n",
    "\n",
    "        websearch_summary = state.get(\"websearch_summary\", [])\n",
    "        web_context = \"\"\n",
    "        if websearch_summary:\n",
    "            web_context = \"\\n\\nWEB RESEARCH:\\n\" + \"\\n\".join(\n",
    "                f\"- {r.get('title','')}: {r.get('link','')}\" for r in websearch_summary[:5]\n",
    "            )\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are refining an existing career plan based on human feedback. Pay attention to the changes and they must be incorporated into the plan.\n",
    "\n",
    "Existing plan:\n",
    "{last_ai_plan}\n",
    "\n",
    "New feedback from user:\n",
    "{user_query}\n",
    "{web_context}\n",
    "\n",
    "Rewrite ONLY the plan, integrating the feedback. Keep it structured, specific, and actionable. Preserve useful content; ONLY modify where needed.\n",
    "\"\"\"\n",
    "        try:\n",
    "            res = llm_call_with_retry_circuit(prompt)\n",
    "            revised = res.text\n",
    "        except Exception as e:\n",
    "            revised = f\"‚ùå Error refining plan: {e}\"\n",
    "\n",
    "        state[\"user_plan\"] = revised\n",
    "        state.setdefault(\"plan_history\", [])\n",
    "        state[\"plan_history\"].append(revised)\n",
    "        state[\"last_user_prompt\"] = user_query\n",
    "\n",
    "        try:\n",
    "            web_results = state.get(\"websearch_summary\") or state.get(\"websearch_results\") or []\n",
    "            thread_procedural_memory.store_thread_data(\n",
    "                thread_id=thread_id,\n",
    "                profile_data=profile,\n",
    "                user_query=user_query,\n",
    "                ai_response=revised,\n",
    "                user_id=user_id,\n",
    "                web_search=web_results,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to store revised plan: {e}\")\n",
    "\n",
    "        messages.append(AIMessage(revised))\n",
    "        messages.append(AIMessage(\"Here's the revised version. Want to refine further?\"))\n",
    "        state[\"messages\"] = messages\n",
    "        return interrupt(\"Here's the revised version. Want to refine further?\")\n",
    "    \n",
    "\n",
    "\n",
    "def career_plan_review_node(state: AgentState) -> dict:\n",
    "    \"\"\"Deprecated: functionality moved into career_plan_combined_node.\"\"\"\n",
    "    return Command(goto=\"career_plan_combined\", update=state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_qa_graph():\n",
    "    \"\"\"Segment 1: Analyze Profile (scrape if needed) ‚Üí Router ‚Üí task nodes; HIL via user_interaction.\"\"\"\n",
    "    builder = StateGraph(AgentState)\n",
    "\n",
    "    # Nodes\n",
    "    builder.add_node(\"linkedin_scraper\", linkedin_scraper_node)\n",
    "    builder.add_node(\"career_qa_router\", career_qa_router)\n",
    "    builder.add_node(\"analyze_profile\", analyze_profile_node)\n",
    "    builder.add_node(\"job_fit_agent\", job_fit_agent_node)\n",
    "    builder.add_node(\"enhance_profile\", enhance_profile_node)\n",
    "    builder.add_node(\"general_qa_node\", general_qa_node)\n",
    "    builder.add_node(\"user_interaction\", user_interaction_node)\n",
    "\n",
    "    # Entry at analyze_profile as per spec\n",
    "    builder.set_entry_point(\"analyze_profile\")\n",
    "\n",
    "    # Scraper ‚Üí router\n",
    "    builder.add_edge(\"linkedin_scraper\", \"career_qa_router\")\n",
    "\n",
    "    # Router ‚Üí nodes are dynamic via Command.go_to (no static fan-out edges)\n",
    "\n",
    "    # Task nodes loop back to router; router will interrupt to await next question\n",
    "    for task in [\"analyze_profile\", \"job_fit_agent\", \"enhance_profile\", \"general_qa_node\"]:\n",
    "        builder.add_edge(task, \"career_qa_router\")\n",
    "\n",
    "    # User interaction loops back to router/analyze as needed dynamically\n",
    "    builder.add_edge(\"user_interaction\", \"career_qa_router\")\n",
    "\n",
    "    return builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "def build_plan_graph():\n",
    "    \"\"\"Single-node combined planner with HIL review cycles via interrupts.\"\"\"\n",
    "    builder = StateGraph(AgentState)\n",
    "\n",
    "    # Nodes\n",
    "    builder.add_node(\"career_plan_combined\", career_plan_combined_node)\n",
    "\n",
    "    # Entry point = combined node\n",
    "    builder.set_entry_point(\"career_plan_combined\")\n",
    "\n",
    "    # Self-loop to allow iterative reviews (the node interrupts each time)\n",
    "    builder.add_edge(\"career_plan_combined\", \"career_plan_combined\")\n",
    "\n",
    "    return builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "\n",
    "# Compile both segments\n",
    "qa_graph = build_qa_graph()\n",
    "plan_graph = build_plan_graph()\n",
    "\n",
    "all = [\"qa_graph\", \"plan_graph\", \"memory\", \"thread_procedural_memory\", \"test_thread_based_learning\"]\n",
    "\n",
    "\n",
    "\n",
    "# def test_thread_based_learning():\n",
    "#     \"\"\"Test the thread-based procedural learning system\"\"\"\n",
    "#     print(\"=== TESTING THREAD-BASED PROCEDURAL LEARNING ===\")\n",
    "    \n",
    "#     # Test profile data\n",
    "#     test_profile = {\n",
    "#         \"headline\": \"Software Engineer\",\n",
    "#         \"skills\": [\"Python\", \"React\", \"AWS\"],\n",
    "#         \"experience\": \"3 years\",\n",
    "#         \"user_id\": \"test_user_123\"\n",
    "#     }\n",
    "    \n",
    "#     # Test storing thread data\n",
    "#     success = thread_procedural_memory.store_thread_data(\n",
    "#         thread_id=\"test_thread_1\",\n",
    "#         profile_data=test_profile,\n",
    "#         user_query=\"I want to transition to data science\",\n",
    "#         ai_response=\"Here's a comprehensive data science transition plan: 1. Learn Python for data science...\",\n",
    "#         user_id=\"test_user_123\"\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Store test: {'√¢≈ì‚Ä¶ PASSED' if success else '√¢¬ù≈í FAILED'}\")\n",
    "    \n",
    "#     # Test retrieval\n",
    "#     similar_threads = thread_procedural_memory.get_similar_threads(\n",
    "#         query=\"data science career transition\",\n",
    "#         user_id=\"test_user_123\"\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Retrieval test: {'√¢≈ì‚Ä¶ PASSED' if similar_threads else '√¢¬ù≈í FAILED'}\")\n",
    "    \n",
    "#     # Debug the test thread\n",
    "#     thread_procedural_memory.debug_thread(\"test_thread_1\")\n",
    "    \n",
    "#     print(\"=== TEST COMPLETE ===\")\n",
    "\n",
    "\n",
    "all = [\"graph\", \"memory\", \"thread_procedural_memory\", \"test_thread_based_learning\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print(\"--- Testing Enhanced Thread-Based Procedural Learning ---\")\n",
    "    # test_thread_based_learning()\n",
    "    \n",
    "    print(\"\\n--- Testing Graph with All Fixes ---\")\n",
    "    \n",
    "    # Test the complete workflow\n",
    "    test_state = {\n",
    "        \"messages\": [HumanMessage(content=\"I want to transition from software engineering to AI/ML\")],\n",
    "        \"profile_data\": {\n",
    "            \"headline\": \"Senior Software Engineer\", \n",
    "            \"skills\": [\"Python\", \"JavaScript\", \"React\"],\n",
    "            \"experience\": \"5 years\",\n",
    "            \"user_id\": \"test_user_integration\"\n",
    "        },\n",
    "        \"linkedin_url\": \"https://linkedin.com/in/testuser\",\n",
    "        \"thread_id\": \"integration_test_thread\",\n",
    "        \"profile_scraped\": True  # Mark as already scraped\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = qa_graph.invoke(\n",
    "            test_state,\n",
    "            config={\"configurable\": {\"thread_id\": \"integration_test_thread\"}}\n",
    "        )\n",
    "        \n",
    "        print(\"√¢≈ì‚Ä¶ Graph execution successful\")\n",
    "        print(f\"Messages generated: {len(result.get('messages', []))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"√¢¬ù≈í Graph execution failed: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d931c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "    # Assuming 'graph' is your compiled LangGraph object\n",
    "display(Image(qa_graph.get_graph().draw_mermaid_png(max_retries=5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
